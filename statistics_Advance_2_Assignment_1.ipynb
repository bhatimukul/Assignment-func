{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY5IVCWqTGG7Cyx5txeNk2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatimukul/Assignment-func/blob/main/statistics_Advance_2_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "ZYhJ9N0N0axG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.** The **F-distribution** is a probability distribution that arises frequently in the analysis of variance (ANOVA), regression analysis, and other statistical tests that compare variances. Below are its key properties:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Definition**:\n",
        "The F-distribution is the distribution of the ratio of two independent chi-squared variables divided by their respective degrees of freedom. Mathematically:\n",
        "\n",
        "\\[\n",
        "F = \\frac{\\left( \\frac{X_1^2}{d_1} \\right)}{\\left( \\frac{X_2^2}{d_2} \\right)}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\( X_1^2 \\) and \\( X_2^2 \\) are independent chi-squared random variables.\n",
        "- \\( d_1 \\) and \\( d_2 \\) are the degrees of freedom of the respective chi-squared variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Shape**:\n",
        "The shape of the F-distribution is determined by its degrees of freedom:\n",
        "- \\( d_1 \\): Degrees of freedom for the numerator.\n",
        "- \\( d_2 \\): Degrees of freedom for the denominator.\n",
        "\n",
        "The shape is skewed to the right, especially for small \\( d_1 \\) and \\( d_2 \\), but becomes more symmetric as \\( d_1 \\) and \\( d_2 \\) increase.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Range**:\n",
        "- The F-distribution is defined for \\( F \\geq 0 \\) because it is based on variances, which cannot be negative.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Asymmetry**:\n",
        "- It is positively skewed. The degree of skewness decreases as the degrees of freedom increase.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Mean**:\n",
        "The mean of the F-distribution is:\n",
        "\\[\n",
        "\\text{Mean} = \\frac{d_2}{d_2 - 2} \\quad \\text{for } d_2 > 2.\n",
        "\\]\n",
        "If \\( d_2 \\leq 2 \\), the mean is undefined.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Variance**:\n",
        "The variance of the F-distribution is:\n",
        "\\[\n",
        "\\text{Variance} = \\frac{2 \\cdot d_2^2 \\cdot (d_1 + d_2 - 2)}{d_1 \\cdot (d_2 - 2)^2 \\cdot (d_2 - 4)} \\quad \\text{for } d_2 > 4.\n",
        "\\]\n",
        "If \\( d_2 \\leq 4 \\), the variance is undefined.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Applications**:\n",
        "The F-distribution is commonly used in:\n",
        "- **Analysis of Variance (ANOVA)**: To test if the means of multiple groups are equal by comparing group variances.\n",
        "- **Regression analysis**: To test the overall significance of a model.\n",
        "- **Equality of variances tests**: Like the Levene's test or Bartlett's test.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Dependency on Degrees of Freedom**:\n",
        "- The degrees of freedom \\( d_1 \\) and \\( d_2 \\) greatly affect the critical values of the F-distribution.\n",
        "- The distribution becomes more symmetric as \\( d_1 \\) and \\( d_2 \\) increase.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Relation to Other Distributions**:\n",
        "- If \\( d_1 = 1 \\), the F-distribution simplifies to a scaled beta distribution.\n",
        "- The F-distribution is related to the t-distribution:\n",
        "  \\[\n",
        "  t^2_{d_2} \\sim F_{1, d_2}.\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Cumulative Distribution Function (CDF)**:\n",
        "The CDF of the F-distribution is not expressible in a simple closed form but is computed using numerical integration of the incomplete beta function.\n",
        "\n",
        "---\n",
        "\n",
        "The F-distribution's properties make it an essential tool for comparing variances and understanding model fit in statistical analysis."
      ],
      "metadata": {
        "id": "w0OiYgCR1A2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "QOy59ftQ1N-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.** The **F-distribution** is used in statistical tests that involve comparing variances or evaluating the significance of models. Its application is grounded in its definition as the ratio of two variances (scaled chi-squared distributions). Below are the primary types of statistical tests where the F-distribution is used and the rationale for its appropriateness:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Analysis of Variance (ANOVA)**:\n",
        "- **Purpose**: To compare the means of two or more groups.\n",
        "- **Why F-distribution is used**:\n",
        "  - In ANOVA, the test statistic is the ratio of the variance between group means (explained variance) to the variance within groups (unexplained variance).\n",
        "  - The F-distribution is appropriate because this ratio follows an F-distribution under the null hypothesis that all group means are equal.\n",
        "- **Example**: Testing whether the average heights of plants under three different fertilizers are equal.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Regression Analysis**:\n",
        "- **Purpose**: To test the overall significance of a regression model.\n",
        "- **Why F-distribution is used**:\n",
        "  - The F-statistic in regression compares the variance explained by the model (regression sum of squares) to the unexplained variance (error sum of squares).\n",
        "  - The F-distribution is suitable because this ratio follows the F-distribution if the null hypothesis (no relationship between predictors and outcome) is true.\n",
        "- **Example**: Testing whether a linear regression model explains a significant amount of variation in sales based on advertising budget.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Equality of Variances**:\n",
        "- **Purpose**: To test whether two or more groups have the same variance.\n",
        "- **Why F-distribution is used**:\n",
        "  - The test statistic is the ratio of sample variances from two independent groups.\n",
        "  - Under the null hypothesis that the group variances are equal, this ratio follows an F-distribution.\n",
        "- **Examples**:\n",
        "  - **Levene's Test**: For testing equality of variances across multiple groups.\n",
        "  - **Bartlett’s Test**: Specifically for testing homogeneity of variances under normality.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Model Comparison (Nested Models)**:\n",
        "- **Purpose**: To compare two models where one is a simplified version of the other (nested models).\n",
        "- **Why F-distribution is used**:\n",
        "  - The F-test assesses whether adding more parameters to a model significantly improves its fit by comparing the variance explained by each model.\n",
        "  - The test statistic (ratio of explained variances) follows the F-distribution under the null hypothesis that the simpler model is sufficient.\n",
        "- **Example**: Testing whether adding an interaction term to a regression model improves predictive performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Tests for Specific Hypotheses in Multivariate Analysis**:\n",
        "- **Purpose**: To evaluate hypotheses in MANOVA, canonical correlation analysis, or similar tests.\n",
        "- **Why F-distribution is used**:\n",
        "  - These tests often involve ratios of explained to unexplained variance, similar to ANOVA, and follow the F-distribution under the null hypothesis.\n",
        "- **Example**: Testing whether the mean vectors of two groups differ across multiple dependent variables.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Two-Sample Variance Comparison (F-Test)**:\n",
        "- **Purpose**: To compare the variances of two independent samples.\n",
        "- **Why F-distribution is used**:\n",
        "  - The test statistic is the ratio of the sample variances, which follows the F-distribution under the null hypothesis that the variances are equal.\n",
        "- **Example**: Comparing variability in test scores between two classes.\n",
        "\n",
        "---\n",
        "\n",
        "### Why the F-Distribution is Appropriate:\n",
        "1. **Ratio of Variances**:\n",
        "   - The F-distribution inherently models ratios of variances, making it ideal for variance-based hypothesis testing.\n",
        "2. **Skewness**:\n",
        "   - It accounts for the fact that variances (and their ratios) cannot be negative.\n",
        "3. **Degrees of Freedom**:\n",
        "   - Its shape adapts based on the degrees of freedom, reflecting the variability introduced by sample size differences.\n",
        "\n",
        "The F-distribution's flexibility and theoretical basis make it indispensable for tests involving variance comparisons and model evaluations."
      ],
      "metadata": {
        "id": "cE9GrUiD1WmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.What are the key assumptions required for conducting an F-test to compare the variances of two**\n",
        "**populations?**"
      ],
      "metadata": {
        "id": "7_Vntlza1uZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.** The **F-test** for comparing the variances of two populations has specific assumptions that must be met to ensure valid results. These assumptions are as follows:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Independence**:\n",
        "- The two samples must be **independent** of each other.\n",
        "- There should be no relationship between the observations in one sample and those in the other.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Normality**:\n",
        "- Both populations should follow a **normal distribution**.\n",
        "- This assumption is crucial because the F-test is sensitive to deviations from normality. If the data are not normally distributed, the test results may be unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Random Sampling**:\n",
        "- The samples must be drawn from their respective populations using a **random sampling method**.\n",
        "- This ensures that the samples are representative of the populations.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Non-Negative Variances**:\n",
        "- Variances are always non-negative, so the test is only meaningful if both sample variances are greater than zero.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Equality of Measurement Scale**:\n",
        "- Both samples must be measured on the same scale (e.g., inches, kilograms) to make their variances comparable.\n",
        "\n",
        "---\n",
        "\n",
        "### Considerations for Violations of Assumptions:\n",
        "1. **Normality**:\n",
        "   - If the normality assumption is violated, use a non-parametric test like **Levene’s test** or the **Brown-Forsythe test**, which are less sensitive to non-normal distributions.\n",
        "2. **Independence**:\n",
        "   - If the samples are not independent, the F-test cannot be used, and alternative methods (e.g., paired tests) should be considered.\n",
        "3. **Outliers**:\n",
        "   - Outliers can heavily influence the variance and, consequently, the F-statistic. It's essential to check for and handle outliers appropriately before conducting the test.\n",
        "\n",
        "---\n",
        "\n",
        "Meeting these assumptions ensures that the F-test provides accurate and meaningful comparisons of population variances."
      ],
      "metadata": {
        "id": "kosfppd42cGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "peX27wBm25k2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.** **Analysis of Variance (ANOVA)** and the **t-test** are both statistical methods used to compare means, but they are suited for different types of problems and have key distinctions. Below is an overview of the purpose of ANOVA and how it differs from the t-test:\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose of ANOVA**\n",
        "ANOVA is used to determine whether there are significant differences between the means of **three or more groups** by analyzing variances. Its key purposes include:\n",
        "\n",
        "1. **Assessing Group Differences**:\n",
        "   - ANOVA tests whether the differences in group means are statistically significant, based on the variance between groups compared to the variance within groups.\n",
        "\n",
        "2. **Partitioning Variance**:\n",
        "   - ANOVA decomposes the total variance in the data into two components:\n",
        "     - **Between-group variance**: Variation due to differences between group means.\n",
        "     - **Within-group variance**: Variation within each group (random noise).\n",
        "\n",
        "3. **Hypothesis Testing**:\n",
        "   - **Null Hypothesis (\\(H_0\\))**: All group means are equal (\\( \\mu_1 = \\mu_2 = \\mu_3 = \\dots \\)).\n",
        "   - **Alternative Hypothesis (\\(H_a\\))**: At least one group mean is different.\n",
        "\n",
        "4. **Applicability**:\n",
        "   - Often used in experiments, clinical trials, and observational studies involving multiple groups or factors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Differences Between ANOVA and the t-Test**\n",
        "\n",
        "| **Feature**              | **t-Test**                                | **ANOVA**                                    |\n",
        "|---------------------------|-------------------------------------------|----------------------------------------------|\n",
        "| **Purpose**               | Compares means of **two groups**.         | Compares means of **three or more groups**.  |\n",
        "| **Type of Variance**      | Does not partition variance explicitly.   | Explicitly partitions variance into between-group and within-group components. |\n",
        "| **Number of Groups**      | Limited to two groups.                   | Can handle three or more groups.             |\n",
        "| **Types of t-Tests**      | Includes independent and paired t-tests.  | Includes one-way and two-way ANOVA.          |\n",
        "| **Null Hypothesis (\\(H_0\\))** | Means of two groups are equal.          | Means of all groups are equal.               |\n",
        "| **Alternative Hypothesis (\\(H_a\\))** | Means are different.                  | At least one mean is different.              |\n",
        "| **Test Statistic**        | Based on the t-distribution.             | Based on the F-distribution.                 |\n",
        "| **Multiple Groups**       | Requires repeated pairwise comparisons (risk of Type I error inflation). | Evaluates all groups simultaneously without inflating the Type I error. |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Each Test**\n",
        "- **Use a t-test**:\n",
        "  - When comparing **exactly two groups**.\n",
        "  - Example: Testing whether the mean test scores of two classes are different.\n",
        "\n",
        "- **Use ANOVA**:\n",
        "  - When comparing **three or more groups**.\n",
        "  - Example: Testing whether three different diets result in different mean weight losses.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of ANOVA Over t-Test**\n",
        "1. **Scalability**:\n",
        "   - ANOVA handles multiple groups simultaneously, reducing the need for multiple pairwise t-tests.\n",
        "   \n",
        "2. **Error Control**:\n",
        "   - Reduces the risk of **Type I error** inflation, which occurs when performing multiple t-tests.\n",
        "\n",
        "3. **Versatility**:\n",
        "   - ANOVA can be extended to include multiple factors (e.g., two-way ANOVA) and interactions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Comparison**\n",
        "- **t-Test Example**:\n",
        "   - Comparing the average heights of males and females.\n",
        "   - Null Hypothesis: \\( \\mu_{\\text{male}} = \\mu_{\\text{female}} \\).\n",
        "\n",
        "- **ANOVA Example**:\n",
        "   - Comparing average heights across three ethnic groups.\n",
        "   - Null Hypothesis: \\( \\mu_1 = \\mu_2 = \\mu_3 \\).\n",
        "\n"
      ],
      "metadata": {
        "id": "9mxD4Yjp3LOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more**\n",
        "**than two groups.**"
      ],
      "metadata": {
        "id": "3RjfCHQr3fxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**You would use a **one-way ANOVA** instead of multiple t-tests when comparing more than two groups for the following reasons:\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use One-Way ANOVA**\n",
        "1. **Number of Groups**:\n",
        "   - You have **three or more groups** to compare. For example, you might be comparing the mean test scores of students taught using three different teaching methods.\n",
        "\n",
        "2. **Single Independent Variable**:\n",
        "   - There is one independent variable (factor) with multiple levels (categories). For example, \"teaching method\" with levels: Method A, Method B, and Method C.\n",
        "\n",
        "3. **Objective**:\n",
        "   - The goal is to determine whether there is a statistically significant difference in the means of the groups, rather than just comparing groups pair by pair.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use One-Way ANOVA Instead of Multiple t-Tests**\n",
        "\n",
        "#### 1. **Error Rate Control**:\n",
        "- **Problem with Multiple t-Tests**:\n",
        "   - Conducting multiple t-tests increases the likelihood of a **Type I error** (false positive). The probability of at least one Type I error increases with the number of t-tests.\n",
        "   - For \\( n \\) groups, the number of pairwise comparisons is \\( \\frac{n(n-1)}{2} \\). For 4 groups, this requires 6 t-tests; for 5 groups, 10 t-tests, and so on.\n",
        "\n",
        "   \\[\n",
        "   \\text{Family-wise error rate (FWER)} = 1 - (1 - \\alpha)^k\n",
        "   \\]\n",
        "   where \\( \\alpha \\) is the significance level (e.g., 0.05) and \\( k \\) is the number of comparisons.\n",
        "\n",
        "- **Solution with ANOVA**:\n",
        "   - One-way ANOVA performs a **single test** to evaluate differences across all groups simultaneously, maintaining the overall significance level (\\( \\alpha \\)).\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Simplicity and Efficiency**:\n",
        "- Conducting multiple t-tests is cumbersome and inefficient as the number of groups increases. One-way ANOVA provides a single test for evaluating all group differences at once.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Comprehensive Analysis**:\n",
        "- **One-way ANOVA** evaluates the overall variance among all groups (using between-group and within-group variance). If the null hypothesis is rejected, it suggests at least one group mean is different but does not indicate which groups differ. Post-hoc tests can then be used for specific group comparisons.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Theoretical Foundation**:\n",
        "- The F-statistic used in one-way ANOVA is derived from a ratio of variances, which provides a robust measure of overall group differences under the assumption of normality and equal variances.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Comparison**\n",
        "\n",
        "**Scenario**: Testing whether three teaching methods (A, B, and C) result in different mean test scores.\n",
        "\n",
        "- **Multiple t-tests**:\n",
        "  - You would perform 3 pairwise tests: A vs. B, B vs. C, and A vs. C.\n",
        "  - Risk: Increased Type I error due to repeated testing.\n",
        "\n",
        "- **One-way ANOVA**:\n",
        "  - A single test evaluates whether the means of all three groups are significantly different.\n",
        "  - Null Hypothesis (\\( H_0 \\)): The means of all groups are equal (\\( \\mu_A = \\mu_B = \\mu_C \\)).\n",
        "  - Alternative Hypothesis (\\( H_a \\)): At least one group mean is different."
      ],
      "metadata": {
        "id": "KI5sM6Vi3ty3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.**\n",
        "**How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "DDqrVRAW4NVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.** In **Analysis of Variance (ANOVA)**, the total variability in the data is partitioned into two components: **between-group variance** and **within-group variance**. This partitioning is central to the calculation of the **F-statistic**, which determines whether there are significant differences between group means.\n",
        "\n",
        "---\n",
        "\n",
        "### **Partitioning of Variance in ANOVA**\n",
        "\n",
        "#### 1. **Total Variance**:\n",
        "The total variance in the data measures the overall variability of all observations from the grand mean. It is quantified by the **Total Sum of Squares (SST)**:\n",
        "\\[\n",
        "\\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y})^2\n",
        "\\]\n",
        "where:\n",
        "- \\( Y_{ij} \\): The \\( j \\)-th observation in the \\( i \\)-th group.\n",
        "- \\( \\bar{Y} \\): The grand mean (mean of all observations across groups).\n",
        "- \\( k \\): Number of groups.\n",
        "- \\( n_i \\): Number of observations in group \\( i \\).\n",
        "\n",
        "#### 2. **Between-Group Variance**:\n",
        "This represents the variability due to differences between group means and is measured by the **Between-Group Sum of Squares (SSB)**:\n",
        "\\[\n",
        "\\text{SSB} = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y})^2\n",
        "\\]\n",
        "where:\n",
        "- \\( \\bar{Y}_i \\): Mean of the \\( i \\)-th group.\n",
        "- \\( n_i \\): Number of observations in the \\( i \\)-th group.\n",
        "\n",
        "It reflects how far each group mean (\\( \\bar{Y}_i \\)) is from the grand mean (\\( \\bar{Y} \\)).\n",
        "\n",
        "#### 3. **Within-Group Variance**:\n",
        "This represents the variability within each group and is measured by the **Within-Group Sum of Squares (SSW)**:\n",
        "\\[\n",
        "\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n",
        "\\]\n",
        "It captures the deviations of individual observations from their group mean (\\( \\bar{Y}_i \\)).\n",
        "\n",
        "---\n",
        "\n",
        "### **Relationship Between Variance Components**:\n",
        "The total variance is the sum of between-group variance and within-group variance:\n",
        "\\[\n",
        "\\text{SST} = \\text{SSB} + \\text{SSW}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Contribution to the F-Statistic**\n",
        "\n",
        "The **F-statistic** in ANOVA is calculated as the ratio of the between-group variance to the within-group variance. This ratio assesses whether the variation between groups is significantly larger than the variation within groups, which would suggest a difference in group means.\n",
        "\n",
        "#### 1. **Mean Squares**:\n",
        "- **Between-Group Mean Square (MSB)**:\n",
        "  \\[\n",
        "  \\text{MSB} = \\frac{\\text{SSB}}{k-1}\n",
        "  \\]\n",
        "  where \\( k-1 \\) is the degrees of freedom for the between-group variability.\n",
        "  \n",
        "- **Within-Group Mean Square (MSW)**:\n",
        "  \\[\n",
        "  \\text{MSW} = \\frac{\\text{SSW}}{N-k}\n",
        "  \\]\n",
        "  where \\( N-k \\) is the degrees of freedom for the within-group variability, with \\( N \\) being the total number of observations.\n",
        "\n",
        "#### 2. **F-Statistic**:\n",
        "The F-statistic is the ratio of these two mean squares:\n",
        "\\[\n",
        "F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
        "\\]\n",
        "\n",
        "- A **large F-value** suggests that the between-group variance is much greater than the within-group variance, indicating significant differences among group means.\n",
        "- A **small F-value** suggests that the between-group variance is comparable to or smaller than the within-group variance, indicating no significant differences among group means.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Variance Partitioning and the F-Statistic**\n",
        "1. The **total variance** (SST) is split into:\n",
        "   - **Between-group variance (SSB)**: Variability due to differences between group means.\n",
        "   - **Within-group variance (SSW)**: Variability due to differences within groups.\n",
        "\n",
        "2. These variances are standardized into mean squares (MSB and MSW).\n",
        "\n",
        "3. The **F-statistic** (\\( F = \\text{MSB} / \\text{MSW} \\)) compares the two, testing whether the variability between groups is significantly larger than the variability within groups.\n",
        "\n",
        "This partitioning allows ANOVA to test the hypothesis that group means are equal in a systematic and interpretable way."
      ],
      "metadata": {
        "id": "NCaqT0Bc4dFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key**\n",
        "d**ifferences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "9QjINTQw40lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The **classical (frequentist) approach** to ANOVA and the **Bayesian approach** to ANOVA differ fundamentally in how they conceptualize and handle uncertainty, parameter estimation, and hypothesis testing. Below is a comparison of the two approaches:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Uncertainty**\n",
        "- **Frequentist Approach**:\n",
        "  - Views probability as the long-run frequency of events.\n",
        "  - Uncertainty is quantified through **p-values** and confidence intervals, which rely on repeated sampling concepts.\n",
        "  - Assumes fixed parameters (e.g., group means and variances) and tests hypotheses about their values using observed data.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Views probability as a degree of belief about a parameter, updated with data.\n",
        "  - Uncertainty is expressed using **posterior distributions**, which combine prior knowledge (or beliefs) with observed data via Bayes’ theorem.\n",
        "  - Parameters are treated as random variables with distributions that reflect uncertainty about their true values.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Parameter Estimation**\n",
        "- **Frequentist Approach**:\n",
        "  - Estimates parameters (e.g., group means and variances) using **point estimates** such as sample means and variances.\n",
        "  - No prior knowledge about the parameters is incorporated; estimation is based solely on the data.\n",
        "  - Does not provide direct probabilities about parameter values (e.g., cannot say \"the mean is likely between X and Y\").\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Estimates parameters by generating **posterior distributions** that reflect uncertainty about parameter values.\n",
        "  - Combines prior distributions (representing prior beliefs) with likelihood derived from the observed data.\n",
        "  - Allows probabilistic statements, such as \"there is a 95% probability that the mean is between X and Y.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Hypothesis Testing**\n",
        "- **Frequentist Approach**:\n",
        "  - Tests the null hypothesis (\\(H_0\\)) that all group means are equal using an F-statistic:\n",
        "    \\[\n",
        "    F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}\n",
        "    \\]\n",
        "  - A small **p-value** (e.g., \\(p < 0.05\\)) leads to rejection of \\(H_0\\), but it does not quantify the probability that \\(H_0\\) is true.\n",
        "  - Results depend on a fixed significance level (\\(\\alpha\\)) and do not incorporate prior information.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Tests hypotheses by comparing **posterior probabilities** or computing **Bayes factors**, which quantify the strength of evidence for competing models (e.g., equal vs. unequal means).\n",
        "    \\[\n",
        "    \\text{Bayes Factor} = \\frac{\\text{Probability of Data under Model 1}}{\\text{Probability of Data under Model 2}}\n",
        "    \\]\n",
        "  - Allows direct probabilities about hypotheses, such as \"there is a 90% probability that the means differ.\"\n",
        "  - Naturally incorporates prior beliefs about hypotheses or parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Interpretation of Results**\n",
        "- **Frequentist Approach**:\n",
        "  - A significant result (small p-value) indicates that the data are unlikely under the null hypothesis, leading to its rejection.\n",
        "  - Provides no direct measure of the probability of the null hypothesis or alternative hypothesis being true.\n",
        "  - Confidence intervals indicate a range of plausible values for parameters but do not have a probabilistic interpretation.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Posterior probabilities directly express the degree of belief in a hypothesis or parameter value.\n",
        "  - Results are probabilistic and provide more intuitive interpretations (e.g., \"there is an 80% probability that the difference between means exceeds 5\").\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Prior Information**\n",
        "- **Frequentist Approach**:\n",
        "  - Does not incorporate prior information; conclusions are based entirely on the observed data.\n",
        "  - Results are objective in the sense that they depend only on the data and the chosen significance level.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Explicitly incorporates prior information through prior distributions.\n",
        "  - Results can vary depending on the chosen priors, which can be subjective or objective.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Computational Requirements**\n",
        "- **Frequentist Approach**:\n",
        "  - Relies on relatively simple calculations (e.g., F-statistic, p-values) and assumes analytical solutions.\n",
        "  - Computationally less intensive, especially for simple designs.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Often requires computationally intensive methods, such as **Markov Chain Monte Carlo (MCMC)**, to approximate posterior distributions.\n",
        "  - More demanding computationally, particularly for complex models.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Applications**\n",
        "- **Frequentist Approach**:\n",
        "  - Standard method for hypothesis testing in many fields, especially when prior information is unavailable or undesirable.\n",
        "  - Commonly used in situations where simplicity and transparency are prioritized.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Preferred when prior information is available and relevant, or when a probabilistic interpretation of results is desired.\n",
        "  - Useful for small-sample studies, hierarchical models, and complex experimental designs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences at a Glance**\n",
        "\n",
        "| **Aspect**              | **Frequentist ANOVA**                           | **Bayesian ANOVA**                              |\n",
        "|--------------------------|------------------------------------------------|------------------------------------------------|\n",
        "| **Uncertainty**          | Based on long-run frequencies (p-values).      | Based on posterior distributions.              |\n",
        "| **Parameter Estimates**  | Point estimates (e.g., means, variances).      | Posterior distributions reflecting uncertainty. |\n",
        "| **Hypothesis Testing**   | Uses p-values to reject \\(H_0\\).               | Computes posterior probabilities or Bayes factors. |\n",
        "| **Incorporates Priors**  | No.                                             | Yes, explicitly.                               |\n",
        "| **Interpretation**       | Indirect (reject \\(H_0\\) if p-value is small). | Direct (probabilities about hypotheses).       |\n",
        "| **Computation**          | Simple and fast.                               | Computationally intensive (e.g., MCMC).        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dVj66b0u4-iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.Question: You have two sets of data representing the incomes of two different professions:**\n",
        "\n",
        ". Profession A: [48, 52, 55, 60, 62'\n",
        "\n",
        ". Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "usMhGV3L5zNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for both professions\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances for both professions\n",
        "var_A = np.var(profession_A, ddof=1)  # Sample variance\n",
        "var_B = np.var(profession_B, ddof=1)  # Sample variance\n",
        "\n",
        "# Calculate the F-statistic (larger variance / smaller variance)\n",
        "F_statistic = var_A / var_B if var_A > var_B else var_B / var_A\n",
        "\n",
        "# Degrees of freedom for both groups\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Calculate the p-value using the F-distribution\n",
        "p_value = 1 - stats.f.cdf(F_statistic, df_A, df_B)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tieHOI0R6n_h",
        "outputId": "f4b94e45-a5b8-4688-856a-e7dba700d4ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.24652429950266952)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-statistic for comparing the variances of the incomes of the two professions is approximately **2.09**, and the corresponding **p-value** is approximately **0.247**.\n",
        "\n",
        "### Interpretation:\n",
        "- The **null hypothesis** (\\(H_0\\)) for the F-test is that the variances of the two populations are equal.\n",
        "- The **alternative hypothesis** (\\(H_a\\)) is that the variances are not equal.\n",
        "\n",
        "Given that the p-value (\\(0.247\\)) is greater than the commonly used significance level of **0.05**, we **fail to reject the null hypothesis**. This means there is not enough evidence to conclude that the variances of the two professions' incomes are significantly different.\n",
        "\n",
        "### Conclusion:\n",
        "Based on the F-test, we conclude that there is no significant difference in the variances of the incomes between Profession A and Profession B."
      ],
      "metadata": {
        "id": "yNynw-k060ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q**9.Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in**\n",
        "**average heights between three different regions with the following data1**\n",
        "\n",
        ". Region A: [160, 162, 165, 158, 164'\n",
        "\n",
        ". Region B: [172, 175, 170, 168, 174'\n",
        "\n",
        ". Region C: [180, 182, 179, 185, 183'\n",
        "\n",
        ". Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "\n",
        ". Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
      ],
      "metadata": {
        "id": "OwqTvhCn658x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Data for the three regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhuH6CuB8PW8",
        "outputId": "60a9760b-b858-4a0c-9c38-564ac3499f75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the one-way ANOVA are as follows:\n",
        "\n",
        "- **F-statistic**: 67.87\n",
        "- **p-value**: \\(2.87 \\times 10^{-7}\\)\n",
        "\n",
        "### Interpretation:\n",
        "- The **null hypothesis** (\\(H_0\\)) is that the average heights across the three regions are equal.\n",
        "- The **alternative hypothesis** (\\(H_a\\)) is that at least one region has a different average height.\n",
        "\n",
        "Since the p-value (\\(2.87 \\times 10^{-7}\\)) is much smaller than the common significance level (\\( \\alpha = 0.05 \\)), we **reject the null hypothesis**.\n",
        "\n",
        "### Conclusion:\n",
        "There is strong evidence to conclude that there are statistically significant differences in the average heights between the three regions. Post-hoc tests (e.g., Tukey's HSD) can be performed to identify which specific regions differ in their average heights."
      ],
      "metadata": {
        "id": "aJ0UC-kL8UL1"
      }
    }
  ]
}