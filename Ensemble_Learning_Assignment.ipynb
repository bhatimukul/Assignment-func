{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfxuEyr/Vc8MCHpf7hjMpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatimukul/Assignment-func/blob/main/Ensemble_Learning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QWWiEc49aMLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Can we use Bagging for regression problems?"
      ],
      "metadata": {
        "id": "CciXqyQeaUh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.Yes, **Bagging (Bootstrap Aggregating)** can be used for **regression problems**. In fact, **Bagging Regressor** is a commonly used ensemble method for improving the stability and accuracy of regression models.\n",
        "\n",
        "### How Bagging Works for Regression:\n",
        "1. **Bootstrap Sampling**: It creates multiple subsets of the training data by randomly sampling with replacement.\n",
        "2. **Train Base Models**: A regression model (e.g., Decision Tree, Linear Regression, etc.) is trained on each subset independently.\n",
        "3. **Aggregation (Averaging Predictions)**: The final prediction is obtained by **averaging** the predictions of all base models.\n",
        "\n",
        "### Benefits of Bagging for Regression:\n",
        "‚úÖ **Reduces Variance**: Especially useful for high-variance models like Decision Trees.  \n",
        "‚úÖ **Improves Accuracy**: Aggregation leads to better generalization.  \n",
        "‚úÖ **Handles Overfitting**: Works well when individual models overfit but their average provides a stable estimate.  \n",
        "\n",
        "### Example: Using `BaggingRegressor` in Scikit-Learn\n",
        "```python\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Base model: Decision Tree Regressor\n",
        "base_model = DecisionTreeRegressor()\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_model, n_estimators=50, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "```\n",
        "### When to Use Bagging for Regression:\n",
        "‚úî If you have a **high-variance model** like Decision Trees.  \n",
        "‚úî When your dataset is **prone to overfitting** and you need better generalization.  \n",
        "‚úî If you want to **improve robustness** without losing too much interpretability.  \n"
      ],
      "metadata": {
        "id": "QhgmFB-FabgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between multiple model training and single model training?"
      ],
      "metadata": {
        "id": "rYshzMTdauEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The key difference between **single model training** and **multiple model training** lies in how the learning process is structured and how predictions are made.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Single Model Training**\n",
        "üîπ **Definition**: Training a single machine learning model on the entire dataset.  \n",
        "üîπ **Example**: Training a **single Decision Tree, Linear Regression, or Neural Network** on your dataset.  \n",
        "üîπ **Workflow**:\n",
        "   - Train one model using all available training data.\n",
        "   - Tune hyperparameters for this one model.\n",
        "   - Make predictions based on the single trained model.\n",
        "\n",
        "üîπ **Pros**:\n",
        "   ‚úÖ Simpler and easier to implement.  \n",
        "   ‚úÖ Faster training and inference (for simple models).  \n",
        "   ‚úÖ Easier to interpret results (especially for models like Decision Trees or Logistic Regression).  \n",
        "\n",
        "üîπ **Cons**:\n",
        "   ‚ùå Can suffer from **high variance** (overfitting) or **high bias** (underfitting).  \n",
        "   ‚ùå May not generalize well on unseen data.  \n",
        "   ‚ùå Less robust to noise or outliers in data.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Multiple Model Training (Ensemble Learning)**\n",
        "üîπ **Definition**: Training multiple models and combining their outputs to improve performance.  \n",
        "üîπ **Types of Multiple Model Training**:\n",
        "   - **Bagging (Bootstrap Aggregating)** ‚Üí Reduces variance by averaging multiple models (e.g., Random Forest).  \n",
        "   - **Boosting** ‚Üí Improves accuracy by focusing on errors iteratively (e.g., XGBoost, AdaBoost, Gradient Boosting).  \n",
        "   - **Stacking** ‚Üí Combines multiple models using a meta-model to make final predictions.  \n",
        "\n",
        "üîπ **Workflow**:\n",
        "   - Train multiple models on different parts of the data (Bagging) or sequentially improve weak models (Boosting).\n",
        "   - Aggregate or combine predictions for better accuracy.\n",
        "\n",
        "üîπ **Pros**:\n",
        "   ‚úÖ **Reduces overfitting** (Bagging helps control variance).  \n",
        "   ‚úÖ **Improves accuracy** by combining multiple weak learners.  \n",
        "   ‚úÖ **More robust** to noise and anomalies.  \n",
        "\n",
        "üîπ **Cons**:\n",
        "   ‚ùå **More computationally expensive** (requires training multiple models).  \n",
        "   ‚ùå **Harder to interpret** compared to a single model.  \n",
        "   ‚ùå **Complex implementation** and tuning of multiple models.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Table**\n",
        "| Feature | Single Model Training | Multiple Model Training (Ensemble) |\n",
        "|---------|----------------------|--------------------------------|\n",
        "| **Number of Models** | 1 | Multiple |\n",
        "| **Complexity** | Simple | More complex |\n",
        "| **Performance** | Depends on the model | Usually better (lower error) |\n",
        "| **Overfitting** | High (in some cases) | Lower (especially with Bagging) |\n",
        "| **Interpretability** | Easy (e.g., Decision Tree, Linear Regression) | Harder (e.g., Random Forest, XGBoost) |\n",
        "| **Computation Cost** | Low | High |\n",
        "| **Robustness** | Can be sensitive to noise | More robust |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use?**\n",
        "‚úî **Single Model** ‚Üí When you need a quick, interpretable solution and computational efficiency.  \n",
        "‚úî **Multiple Models (Ensemble Learning)** ‚Üí When you need **higher accuracy, lower variance, and better generalization** (especially in complex problems like image classification, NLP, or finance).  \n"
      ],
      "metadata": {
        "id": "7RbKHzp3azLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explain the concept of feature randomness in Random Forest?"
      ],
      "metadata": {
        "id": "k-GHElT_a_C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Feature Randomness in Random Forest üå≤üé≤**  \n",
        "\n",
        "**Feature randomness** in **Random Forest** refers to the process of selecting a **random subset of features** at each split while building decision trees. This randomness helps in making the ensemble model more diverse and reduces overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Feature Randomness Works**  \n",
        "\n",
        "1. **Dataset Bootstrapping (Row Sampling)**  \n",
        "   - Random Forest first creates multiple **bootstrap samples** by randomly selecting rows (data points) with replacement.\n",
        "   \n",
        "2. **Feature Selection at Each Split (Column Sampling)**  \n",
        "   - Unlike a normal Decision Tree (which considers all features at every split), **Random Forest selects a random subset of features at each split**.  \n",
        "   - This means that every decision tree in the forest is trained on a different combination of features, introducing diversity.  \n",
        "   - The number of randomly chosen features is controlled by the hyperparameter **`max_features`**.  \n",
        "\n",
        "3. **Tree Growth and Aggregation**  \n",
        "   - Each tree grows independently and makes predictions.  \n",
        "   - The final prediction is made by **majority voting (classification)** or **averaging (regression)** across all trees.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Why Feature Randomness is Useful?**\n",
        "‚úÖ **Reduces Overfitting** ‚Üí Since each tree only considers a subset of features, they don‚Äôt learn the same patterns, making the model more generalizable.  \n",
        "‚úÖ **Increases Diversity** ‚Üí Each tree sees different features, leading to a variety of decision rules, which improves ensemble performance.  \n",
        "‚úÖ **Handles Correlated Features Better** ‚Üí If some features are highly correlated, Random Forest ensures no single feature dominates decision-making.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example: Feature Randomness in Python**\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train Random Forest with feature randomness\n",
        "rf = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Display feature importance\n",
        "print(rf.feature_importances_)\n",
        "```\n",
        "üîπ Here, **`max_features=\"sqrt\"`** means that at each split, the model randomly selects **‚àö(number of features)**.  \n",
        "üîπ If `max_features=None`, the model would behave like Bagging (using all features at each split).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Hyperparameters for Feature Randomness**\n",
        "| Hyperparameter | Description |\n",
        "|---------------|-------------|\n",
        "| **`max_features=\"sqrt\"`** | Default for classification; selects ‚àö(total features) at each split. |\n",
        "| **`max_features=\"log2\"`** | Selects log‚ÇÇ(total features) at each split. |\n",
        "| **`max_features=None`** | Uses all features (like a regular decision tree). |\n",
        "| **`max_features=k`** | Manually set the number of features to consider at each split. |\n"
      ],
      "metadata": {
        "id": "v286bnZNbDas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What is OOB (Out-of-Bag) Score?"
      ],
      "metadata": {
        "id": "0MHtjkH0bQ1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **OOB (Out-of-Bag) Score in Random Forest üå≤üéí**  \n",
        "\n",
        "The **Out-of-Bag (OOB) Score** is an internal validation method used in **Random Forest** to estimate the model's performance **without using a separate validation set**.\n",
        "\n",
        "---\n",
        "\n",
        "### **How OOB Works?**\n",
        "1. **Bootstrap Sampling**:\n",
        "   - Random Forest trains each decision tree on a **random subset** (bootstrapped sample) of the training data.  \n",
        "   - This means that **some data points (around 37%) are left out** of each individual tree's training.\n",
        "\n",
        "2. **Making Predictions on OOB Samples**:\n",
        "   - The trees that **did not see** a particular data point during training can be used to **predict** it.\n",
        "   - The final OOB prediction is obtained by averaging (for regression) or majority voting (for classification) across all trees.\n",
        "\n",
        "3. **Computing the OOB Score**:\n",
        "   - Once all OOB predictions are collected, the accuracy (for classification) or R¬≤ score (for regression) is computed.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why OOB Score is Useful?**\n",
        "‚úÖ **Acts as a Built-in Cross-Validation** ‚Üí No need for a separate validation set, saving data.  \n",
        "‚úÖ **Less Computation Overhead** ‚Üí Faster than techniques like K-Fold Cross-Validation.  \n",
        "‚úÖ **Gives a Reliable Estimate of Generalization Performance** ‚Üí Helps detect overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example: Computing OOB Score in Python**\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train Random Forest with OOB score enabled\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf.oob_score_:.4f}\")\n",
        "```\n",
        "üîπ **`oob_score=True`** enables the OOB estimation during training.  \n",
        "üîπ **`rf.oob_score_`** stores the OOB accuracy (for classification) or R¬≤ score (for regression).  \n",
        "\n",
        "---\n",
        "\n",
        "### **When Should You Use OOB Score?**\n",
        "‚úî When you have **limited data** and don‚Äôt want to set aside a validation set.  \n",
        "‚úî When using **Random Forest**, as it naturally supports OOB scoring.  \n",
        "‚úî When you want a **quick performance estimate** without doing cross-validation.  \n"
      ],
      "metadata": {
        "id": "xQu4ZG3UbVFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How can you measure the importance of features in a Random Forest model?"
      ],
      "metadata": {
        "id": "vI04XLpQbeHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Measuring Feature Importance in a Random Forest Model üå≤‚ú®**  \n",
        "\n",
        "Feature importance helps us understand which features contribute the most to a Random Forest model‚Äôs predictions. **Random Forest provides two main ways** to measure feature importance:\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Mean Decrease in Impurity (MDI) ‚Äì Gini Importance**\n",
        "üîπ **Concept**: Measures how much a feature **reduces impurity (Gini or variance)** across all trees in the forest.  \n",
        "üîπ **Calculation**:  \n",
        "   - Every time a feature is used in a tree split, the reduction in impurity (e.g., Gini index or MSE) is recorded.  \n",
        "   - These reductions are **averaged** across all trees to determine feature importance.  \n",
        "\n",
        "üîπ **Pros**:\n",
        "   ‚úÖ Fast and easy to compute  \n",
        "   ‚úÖ Provides insight into feature contribution  \n",
        "\n",
        "üîπ **Cons**:\n",
        "   ‚ùå **Biased towards high-cardinality features** (features with many unique values tend to have higher importance)  \n",
        "   ‚ùå **Not reliable when features are correlated**  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Permutation Importance ‚Äì Mean Decrease in Accuracy (MDA)**\n",
        "üîπ **Concept**: Measures how much the model‚Äôs **accuracy drops** when a feature‚Äôs values are randomly shuffled.  \n",
        "üîπ **Calculation**:\n",
        "   - The model‚Äôs original accuracy is recorded.  \n",
        "   - The values of a single feature are randomly shuffled (breaking the relationship with the target).  \n",
        "   - The model is re-evaluated, and the drop in accuracy is measured.  \n",
        "   - A larger accuracy drop means the feature is more important.  \n",
        "\n",
        "üîπ **Pros**:\n",
        "   ‚úÖ Works well with correlated features  \n",
        "   ‚úÖ More reliable than MDI for interpreting feature effects  \n",
        "\n",
        "üîπ **Cons**:\n",
        "   ‚ùå Computationally expensive (requires multiple re-evaluations)  \n",
        "   ‚ùå Not useful if the dataset is too small  \n",
        "\n",
        "---\n",
        "\n",
        "## **Example: Compute Feature Importance in Python**\n",
        "### **Method 1: Using `feature_importances_` (MDI)**\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "feature_names = load_iris().feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Extract feature importance\n",
        "importance = rf.feature_importances_\n",
        "\n",
        "# Display feature importance\n",
        "df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importance})\n",
        "df = df.sort_values(by=\"Importance\", ascending=False)\n",
        "print(df)\n",
        "```\n",
        "üîπ The **higher the importance value**, the more influential the feature is.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Method 2: Using Permutation Importance (MDA)**\n",
        "```python\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Compute permutation importance\n",
        "perm_importance = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n",
        "\n",
        "# Display importance\n",
        "df_perm = pd.DataFrame({\"Feature\": feature_names, \"Importance\": perm_importance.importances_mean})\n",
        "df_perm = df_perm.sort_values(by=\"Importance\", ascending=False)\n",
        "print(df_perm)\n",
        "```\n",
        "üîπ **Permutation importance is better when features are correlated**, while `feature_importances_` is faster.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Each Method?**\n",
        "| **Method** | **Best For** | **Weakness** |\n",
        "|------------|-------------|--------------|\n",
        "| **MDI (Gini Importance)** | Fast insights into feature contribution | Biased towards high-cardinality features |\n",
        "| **Permutation Importance (MDA)** | Works well with correlated features | Computationally expensive |\n"
      ],
      "metadata": {
        "id": "LDw1ZD2Hbifr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Explain the working principle of a Bagging Classifier?"
      ],
      "metadata": {
        "id": "YLpgZcDDb0SU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Bagging Classifier: Working Principle üéíüå≤**  \n",
        "\n",
        "A **Bagging Classifier** is an **ensemble learning method** that improves classification accuracy and reduces overfitting by combining multiple weak models (base learners). It is based on **Bootstrap Aggregating (Bagging)**.\n",
        "\n",
        "---\n",
        "\n",
        "## **How the Bagging Classifier Works?**\n",
        "1Ô∏è‚É£ **Bootstrap Sampling (Data Randomization)**  \n",
        "   - The training dataset is **randomly sampled with replacement** to create multiple subsets of the original data.  \n",
        "   - Each subset has the same size as the original dataset but contains **some duplicate samples** and **some missing samples** (~37% of original data is left out in each subset).  \n",
        "\n",
        "2Ô∏è‚É£ **Train Multiple Base Models (Parallel Training)**  \n",
        "   - Each subset is used to train an **independent** model (usually a weak learner like a Decision Tree).  \n",
        "   - These base models **do not interact with each other** and are trained in parallel.  \n",
        "\n",
        "3Ô∏è‚É£ **Aggregation (Voting for Classification)**  \n",
        "   - Each model makes a prediction for a given input.  \n",
        "   - The final output is determined by **majority voting** (for classification) or **averaging** (for regression).  \n",
        "\n",
        "---\n",
        "\n",
        "## **Key Benefits of Bagging Classifier**\n",
        "‚úÖ **Reduces Overfitting** ‚Üí A single Decision Tree may overfit, but multiple trees reduce variance.  \n",
        "‚úÖ **Improves Stability** ‚Üí Works well on noisy data, as it reduces the impact of outliers.  \n",
        "‚úÖ **Handles High Variance Models** ‚Üí Ideal for weak learners like Decision Trees.  \n",
        "‚úÖ **Parallel Processing** ‚Üí Can be trained in parallel, making it computationally efficient.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Bagging Classifier in Python (Using Scikit-Learn)**\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split data into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base model (weak learner)\n",
        "base_model = DecisionTreeClassifier()\n",
        "\n",
        "# Create Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "üîπ **`n_estimators=50`** ‚Üí Trains 50 Decision Trees.  \n",
        "üîπ **Majority voting is used** to make final predictions.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison: Bagging vs. Boosting**\n",
        "| Feature | **Bagging** | **Boosting** |\n",
        "|---------|------------|-------------|\n",
        "| **Model Independence** | Independent models | Sequential models (each model learns from errors of the previous one) |\n",
        "| **Goal** | Reduce variance (better stability) | Reduce bias (better accuracy) |\n",
        "| **Training Process** | Parallel training | Sequential training |\n",
        "| **Overfitting** | Less likely | More prone to overfitting if not regularized |\n",
        "| **Example Models** | Bagging Classifier, Random Forest | AdaBoost, Gradient Boosting, XGBoost |\n"
      ],
      "metadata": {
        "id": "J4BERLJGb63k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.How do you evaluate a Bagging Classifier‚Äôs performance?"
      ],
      "metadata": {
        "id": "XrTp_RRYcEtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Evaluating a Bagging Classifier‚Äôs Performance üéí‚úÖ**  \n",
        "\n",
        "To measure the effectiveness of a **Bagging Classifier**, we use standard classification metrics along with ensemble-specific techniques like **Out-of-Bag (OOB) Score**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Standard Performance Metrics**  \n",
        "Depending on whether it's a **binary** or **multi-class classification** problem, you can use the following metrics:\n",
        "\n",
        "### **‚úî Accuracy (for balanced datasets)**\n",
        "Measures the proportion of correct predictions.  \n",
        "\\[\n",
        "Accuracy = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
        "\\]\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **‚úî Precision, Recall, and F1-Score (for imbalanced datasets)**\n",
        "For cases where class distribution is skewed:\n",
        "- **Precision**: Measures how many predicted positives are actually positive.  \n",
        "- **Recall (Sensitivity)**: Measures how many actual positives were correctly predicted.  \n",
        "- **F1-Score**: The harmonic mean of Precision and Recall.  \n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate detailed classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **‚úî Confusion Matrix**\n",
        "A **confusion matrix** provides insights into how well the model differentiates between classes.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Out-of-Bag (OOB) Score (Built-in Cross-Validation)**\n",
        "Since Bagging uses **bootstrap sampling**, some data points are left out of each tree‚Äôs training set (~37% of data per tree).  \n",
        "These **OOB samples** can be used to estimate the model's performance **without needing a validation set**.\n",
        "\n",
        "```python\n",
        "# Enable OOB score in Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(base_model, n_estimators=50, oob_score=True, random_state=42)\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {bagging_clf.oob_score_:.4f}\")\n",
        "```\n",
        "üîπ **OOB Score ‚âà Cross-Validation Accuracy**, so it helps validate performance without splitting the data.\n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ ROC Curve & AUC Score (for Probabilistic Models)**\n",
        "For models that provide probability outputs, ROC-AUC measures how well the classifier separates classes.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get probability predictions\n",
        "y_probs = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "üîπ **Higher AUC (closer to 1.0) ‚Üí Better classifier performance.**  \n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table: Evaluation Methods**\n",
        "| **Metric** | **Use Case** | **Code Example** |\n",
        "|------------|-------------|-----------------|\n",
        "| **Accuracy** | Balanced datasets | `accuracy_score(y_test, y_pred)` |\n",
        "| **Precision, Recall, F1-Score** | Imbalanced datasets | `classification_report(y_test, y_pred)` |\n",
        "| **Confusion Matrix** | Identify misclassifications | `confusion_matrix(y_test, y_pred)` |\n",
        "| **OOB Score** | Internal cross-validation (no extra validation set needed) | `bagging_clf.oob_score_` |\n",
        "| **ROC-AUC Score** | Probabilistic models (binary classification) | `roc_curve(y_test, y_probs)` |"
      ],
      "metadata": {
        "id": "IM6ltkvUcKzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. How does a Bagging Regressor work?"
      ],
      "metadata": {
        "id": "BpY7_djfcVfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Bagging Regressor: Working Principle üéíüìà**  \n",
        "\n",
        "A **Bagging Regressor** is an ensemble learning method that improves the accuracy and stability of regression models by combining multiple weak regressors. It follows the same **Bootstrap Aggregating (Bagging)** concept used in classification but for regression problems.\n",
        "\n",
        "---\n",
        "\n",
        "## **How a Bagging Regressor Works?**\n",
        "1Ô∏è‚É£ **Bootstrap Sampling (Data Randomization)**  \n",
        "   - The training dataset is **randomly sampled with replacement** to create multiple subsets.  \n",
        "   - Each subset has the same size as the original dataset but contains **some repeated** and **some missing** samples (~37% of the original data is left out per subset).  \n",
        "\n",
        "2Ô∏è‚É£ **Train Multiple Base Regressors**  \n",
        "   - Each bootstrapped subset is used to train an **independent weak regressor** (e.g., Decision Tree Regressor, Linear Regression, etc.).  \n",
        "   - These regressors **do not interact** with each other and are trained **in parallel**.  \n",
        "\n",
        "3Ô∏è‚É£ **Aggregation (Averaging for Regression)**  \n",
        "   - The final prediction is computed by **averaging** the predictions of all individual regressors:  \n",
        "     \\[\n",
        "     \\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{y}_i\n",
        "     \\]\n",
        "   - This reduces variance and improves generalization.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Key Benefits of Bagging Regressor**\n",
        "‚úÖ **Reduces Overfitting** ‚Üí Stabilizes high-variance models (e.g., Decision Trees).  \n",
        "‚úÖ **Handles Outliers** ‚Üí Averaging reduces the impact of extreme values.  \n",
        "‚úÖ **Works Well for Non-Linear Data** ‚Üí Can be used with complex models like Decision Tree Regressors.  \n",
        "‚úÖ **Parallel Training** ‚Üí Each base model is trained independently, making it efficient.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Bagging Regressor in Python (Using Scikit-Learn)**\n",
        "```python\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split data into train-test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base model (weak learner)\n",
        "base_model = DecisionTreeRegressor()\n",
        "\n",
        "# Create Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.4f}\")\n",
        "```\n",
        "üîπ **`n_estimators=50`** ‚Üí Trains 50 Decision Tree Regressors.  \n",
        "üîπ **Final prediction is the average of all trees‚Äô predictions.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison: Bagging Regressor vs. Boosting Regressor**\n",
        "| Feature | **Bagging Regressor** | **Boosting Regressor** |\n",
        "|---------|------------------|------------------|\n",
        "| **Model Independence** | Independent models | Sequential models (each model learns from previous errors) |\n",
        "| **Goal** | Reduce variance | Reduce bias |\n",
        "| **Training Process** | Parallel training | Sequential training |\n",
        "| **Overfitting** | Less likely | More prone if not regularized |\n",
        "| **Example Models** | Bagging Regressor, Random Forest Regressor | AdaBoost Regressor, Gradient Boosting, XGBoost |\n",
        "\n"
      ],
      "metadata": {
        "id": "6vM-XsFvcaIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the main advantage of ensemble techniques?"
      ],
      "metadata": {
        "id": "GrCmFoM-cqIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Main Advantage of Ensemble Techniques üöÄ**  \n",
        "\n",
        "The primary advantage of **ensemble techniques** is that they **combine multiple weak models** to create a **stronger, more robust model** that improves accuracy, reduces overfitting, and enhances generalization.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Key Advantages of Ensemble Techniques:**\n",
        "1Ô∏è‚É£ **Higher Accuracy & Performance**  \n",
        "   - Combining multiple models reduces errors and improves predictive performance.  \n",
        "   - Example: **Random Forest** outperforms a single Decision Tree by reducing variance.  \n",
        "\n",
        "2Ô∏è‚É£ **Reduces Overfitting (Variance Reduction)**  \n",
        "   - Individual models (e.g., Decision Trees) may overfit the training data.  \n",
        "   - Bagging (like in Random Forest) helps by averaging predictions, making them more stable.  \n",
        "\n",
        "3Ô∏è‚É£ **Handles Noisy Data & Outliers**  \n",
        "   - Boosting techniques (like AdaBoost, Gradient Boosting) **focus on misclassified samples**, making models more robust.  \n",
        "   - Bagging reduces the impact of extreme values by **aggregating predictions**.  \n",
        "\n",
        "4Ô∏è‚É£ **Works Well with Different Models (Heterogeneous Ensembles)**  \n",
        "   - Stacking can combine different types of models (e.g., SVM + Decision Tree + Neural Network) to leverage their strengths.  \n",
        "\n",
        "5Ô∏è‚É£ **Reduces Bias & Variance (Bias-Variance Tradeoff)**  \n",
        "   - **Bagging** reduces **variance** (good for high-variance models like Decision Trees).  \n",
        "   - **Boosting** reduces **bias** (good for underfitting models like Linear Regression).  \n",
        "\n",
        "6Ô∏è‚É£ **Better Generalization to Unseen Data**  \n",
        "   - Ensemble methods generalize well to new data compared to single models.  \n",
        "   - Example: A Random Forest will perform better than a single Decision Tree on test data.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Example: Ensemble vs. Single Model Performance**\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Random Forest (Ensemble)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")  # Higher accuracy due to ensemble effect\n",
        "```\n",
        "üîπ The **Random Forest (ensemble)** will have higher accuracy than a **single Decision Tree** because it reduces variance and improves stability.  \n",
        "\n",
        "---\n",
        "\n",
        "### **When Should You Use Ensemble Techniques?**\n",
        "‚úÖ When a single model has **high variance or overfits** (e.g., Decision Trees).  \n",
        "‚úÖ When you want to **improve accuracy** without increasing complexity.  \n",
        "‚úÖ When you need a **robust model** that generalizes well to new data.  \n"
      ],
      "metadata": {
        "id": "stu3flgicw6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.What is the main challenge of ensemble methods?"
      ],
      "metadata": {
        "id": "-rOLvXuSc72V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Main Challenges of Ensemble Methods ‚ö†Ô∏è**  \n",
        "\n",
        "While **ensemble methods** improve accuracy and stability, they come with certain challenges. Here are the key difficulties:  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ Increased Computational Cost & Training Time ‚è≥üíª**  \n",
        "üîπ **Why?**  \n",
        "- Ensembles train multiple models instead of one, requiring more **CPU/GPU power**.  \n",
        "- **Boosting methods (e.g., Gradient Boosting, XGBoost)** train sequentially, increasing training time.  \n",
        "- **Stacking ensembles** require additional layers of models, making them slower.  \n",
        "\n",
        "üîπ **Solution:**  \n",
        "‚úÖ Use **parallel processing** for Bagging-based methods (e.g., Random Forest).  \n",
        "‚úÖ Optimize parameters (e.g., **reduce `n_estimators`** in Bagging/Boosting).  \n",
        "‚úÖ Use efficient implementations like **XGBoost, LightGBM**, or **distributed computing**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Model Interpretability & Complexity ü§Ø**  \n",
        "üîπ **Why?**  \n",
        "- Ensembles like **Random Forest, XGBoost, and Stacking** are **black-box models**, meaning they are hard to interpret.  \n",
        "- Unlike a single **Decision Tree** (which shows clear rules), ensembles make decisions based on multiple models, making it difficult to explain how a prediction was made.  \n",
        "\n",
        "üîπ **Solution:**  \n",
        "‚úÖ Use **feature importance** techniques (e.g., `feature_importances_` in Random Forest).  \n",
        "‚úÖ Use **SHAP (SHapley Additive Explanations)** or **LIME** for model interpretability.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Risk of Overfitting (Especially in Boosting) üéØ**  \n",
        "üîπ **Why?**  \n",
        "- **Boosting methods (e.g., AdaBoost, Gradient Boosting)** focus heavily on misclassified points, which can lead to **overfitting** on noisy datasets.  \n",
        "- Complex ensembles with too many trees/models can **memorize training data** instead of generalizing.  \n",
        "\n",
        "üîπ **Solution:**  \n",
        "‚úÖ **Regularization** (Use `learning_rate` in boosting, `max_depth` in Random Forest).  \n",
        "‚úÖ **Early Stopping** (Stop training when validation performance stops improving).  \n",
        "‚úÖ **Pruning Trees** (Reduce tree depth to prevent memorization).  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Data Dependency & Poor Performance on Small Datasets üìâ**  \n",
        "üîπ **Why?**  \n",
        "- Ensemble methods work best with **large datasets**; on small datasets, they may not significantly improve over single models.  \n",
        "- If there‚Äôs too little data, Bagging **may not capture diverse patterns**, and Boosting **may overfit**.  \n",
        "\n",
        "üîπ **Solution:**  \n",
        "‚úÖ Use simpler models like **Logistic Regression, Decision Trees, or k-NN** on small datasets.  \n",
        "‚úÖ Use **cross-validation** and OOB scoring to validate performance.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Hyperparameter Tuning is Difficult ‚öôÔ∏è**  \n",
        "üîπ **Why?**  \n",
        "- Ensembles have **many hyperparameters** (e.g., `n_estimators`, `max_depth`, `learning_rate` in boosting).  \n",
        "- Finding the best settings requires expensive techniques like **Grid Search or Bayesian Optimization**.  \n",
        "\n",
        "üîπ **Solution:**  \n",
        "‚úÖ Use **Randomized Search** or **Bayesian Optimization** (`Optuna` for XGBoost).  \n",
        "‚úÖ Start with default values and adjust key parameters first (e.g., `n_estimators`, `max_depth`).  \n",
        "\n",
        "---\n",
        "\n",
        "### **üîé Summary: Challenges & Solutions**\n",
        "| **Challenge** | **Why it Happens?** | **Solution** |\n",
        "|--------------|--------------------|-------------|\n",
        "| **Computational Cost** | Multiple models require more processing power. | Use parallel processing, distributed computing, optimized libraries (e.g., XGBoost, LightGBM). |\n",
        "| **Interpretability** | Ensembles are black-box models. | Use feature importance, SHAP, or LIME for explanations. |\n",
        "| **Overfitting (Boosting)** | Models focus too much on misclassified points. | Use regularization, early stopping, and pruning. |\n",
        "| **Data Dependency** | Works best with large datasets. | Use simpler models for small datasets or validate with cross-validation. |\n",
        "| **Hyperparameter Tuning** | Many parameters to optimize. | Use Randomized Search, Bayesian Optimization (Optuna). |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EsG-rpWkdDPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.Explain the key idea behind ensemble techniques?"
      ],
      "metadata": {
        "id": "dXc73g5wdQvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Key Idea Behind Ensemble Techniques üß©**\n",
        "\n",
        "The core idea behind **ensemble techniques** is that **combining multiple models** (often called **weak learners**) can **boost performance** and produce a **stronger, more reliable model**. The underlying principle is based on the idea that individual models have their own strengths and weaknesses, and by aggregating their predictions, we can **reduce errors** and **improve generalization**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Use Ensemble Methods?**\n",
        "\n",
        "1Ô∏è‚É£ **Leverage Multiple Weak Learners**  \n",
        "   - A **weak learner** is a model that performs slightly better than random guessing (e.g., a single decision tree).  \n",
        "   - **Ensemble methods** combine multiple weak models to create a **stronger learner** that performs well on unseen data.  \n",
        "   - For example, a **Decision Tree** alone might overfit the training data, but combining many trees (as in **Random Forest**) helps reduce overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "2Ô∏è‚É£ **Bias-Variance Tradeoff**  \n",
        "   - **Bias** refers to error due to overly simplistic models that don‚Äôt capture the underlying patterns.  \n",
        "   - **Variance** refers to error due to models that are too complex and sensitive to small fluctuations in the data (e.g., overfitting).  \n",
        "   - **Ensemble techniques** aim to **balance bias and variance**, combining models to reduce **high variance** (overfitting) or **high bias** (underfitting) depending on the approach.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Types of Ensemble Methods**\n",
        "\n",
        "1Ô∏è‚É£ **Bagging (Bootstrap Aggregating)**  \n",
        "   - **Idea**: Build multiple independent models (usually of the same type) using different subsets of the training data. The final prediction is made by **averaging** (regression) or **majority voting** (classification) from all models.  \n",
        "   - **Main Advantage**: Reduces variance and overfitting.  \n",
        "   - **Example**: **Random Forest** (using decision trees as base learners).\n",
        "\n",
        "2Ô∏è‚É£ **Boosting**  \n",
        "   - **Idea**: Build models sequentially, with each new model focusing on the errors (misclassifications or residuals) of the previous model. The final prediction is a weighted average of all models.  \n",
        "   - **Main Advantage**: Reduces bias and increases model accuracy.  \n",
        "   - **Example**: **AdaBoost**, **Gradient Boosting**, **XGBoost**.\n",
        "\n",
        "3Ô∏è‚É£ **Stacking (Stacked Generalization)**  \n",
        "   - **Idea**: Combine predictions from multiple models (can be of different types) and use a **meta-model** to learn how best to combine them. The meta-model takes the predictions of base models as features and learns the optimal combination.  \n",
        "   - **Main Advantage**: Leverages the strengths of different types of models.  \n",
        "   - **Example**: Combining Decision Trees, SVM, and Logistic Regression using a Logistic Regression meta-model.\n",
        "\n",
        "4Ô∏è‚É£ **Voting Classifier/Regressor**  \n",
        "   - **Idea**: A simple ensemble method where predictions from multiple models are combined by **majority voting** (for classification) or **averaging** (for regression).  \n",
        "   - **Main Advantage**: Works well when combining models of different types.  \n",
        "   - **Example**: Combine **Logistic Regression**, **SVM**, and **Random Forest** for classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Ensemble Strategy Insights**\n",
        "- **Bagging** is useful when you have **high-variance models** (like decision trees) and want to reduce overfitting.  \n",
        "- **Boosting** works best when you need to **increase accuracy** and reduce bias by focusing on harder-to-classify examples.  \n",
        "- **Stacking** is used when you want to leverage the strengths of different models and combine them intelligently.  \n",
        "- **Voting** is simpler and often used when you have **diverse models** (e.g., Decision Trees and KNN) and want to aggregate their predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary: The Key Idea of Ensemble Methods**\n",
        "The key idea behind ensemble methods is to **combine multiple individual models** to create a stronger overall model that:\n",
        "- **Reduces model bias** and **variance** (depending on the method).\n",
        "- **Improves predictive accuracy** by leveraging the collective knowledge of multiple models.\n",
        "- **Boosts robustness** by compensating for the weaknesses of individual models.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8JWAnBFPdVPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What is a Random Forest Classifier?"
      ],
      "metadata": {
        "id": "iSY91STEduao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **What is a Random Forest Classifier? üå≤üå≤**\n",
        "\n",
        "A **Random Forest Classifier** is an ensemble learning method used for classification tasks. It builds a large number of **decision trees** during training and outputs the **mode (majority vote)** of the individual trees' predictions for classification. This technique combines the predictions of many weak learners (individual decision trees) to create a **stronger, more accurate model**.\n",
        "\n",
        "---\n",
        "\n",
        "## **How Does a Random Forest Classifier Work?**\n",
        "\n",
        "1Ô∏è‚É£ **Bootstrap Sampling (Bagging)**  \n",
        "   - A **bootstrap sample** is a random subset of the training data created by sampling **with replacement**.  \n",
        "   - **Multiple** decision trees are trained on different bootstrap samples of the data, meaning each tree is trained on a **slightly different subset of data**.  \n",
        "   - Each tree learns independently, and there‚Äôs diversity between them.\n",
        "\n",
        "2Ô∏è‚É£ **Feature Randomness (Random Feature Selection)**  \n",
        "   - At each split in the decision tree, the algorithm doesn't consider all features but only a **random subset** of features.  \n",
        "   - This helps to reduce correlation between trees, ensuring that each tree learns from different aspects of the data and preventing overfitting.\n",
        "\n",
        "3Ô∏è‚É£ **Decision Tree Construction**  \n",
        "   - Each tree is grown **deep** (until no further splits improve performance), and the **split criterion** is usually **Gini Impurity** or **Entropy** for classification tasks.  \n",
        "   - Unlike a single decision tree, a random forest uses many trees (typically hundreds or thousands) to make the final prediction.\n",
        "\n",
        "4Ô∏è‚É£ **Voting Mechanism (Majority Voting)**  \n",
        "   - Once the forest is trained, each individual tree makes a **prediction**.  \n",
        "   - For classification, the **Random Forest Classifier** predicts the class that has the most votes from all individual trees (majority voting).\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Advantages of Random Forest Classifier**  \n",
        "‚úÖ **Reduced Overfitting**: Random Forest reduces the risk of overfitting compared to a single decision tree because of bagging and randomness.  \n",
        "‚úÖ **Higher Accuracy**: By aggregating many decision trees, it achieves a more accurate prediction.  \n",
        "‚úÖ **Handles Missing Data**: It can handle missing data by using **surrogate splits** and can still perform well even with incomplete datasets.  \n",
        "‚úÖ **Feature Importance**: It can be used to calculate feature importance, helping to understand which features are most useful for prediction.  \n",
        "‚úÖ **Robustness**: Random Forest is robust to noisy data and outliers because the averaging reduces the impact of such points.\n",
        "\n",
        "---\n",
        "\n",
        "## **Random Forest Classifier Example in Python**\n",
        "\n",
        "Here's how you can use a Random Forest Classifier with **scikit-learn**:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Parameters of Random Forest Classifier**\n",
        "- **`n_estimators`**: Number of trees in the forest (higher is usually better, but computationally expensive).  \n",
        "- **`max_depth`**: Maximum depth of the individual trees (prevents overfitting).  \n",
        "- **`min_samples_split`**: Minimum number of samples required to split an internal node.  \n",
        "- **`max_features`**: Number of features to consider when looking for the best split (a smaller number creates more diversity in trees).  \n",
        "- **`random_state`**: Controls the randomness of the algorithm, making it reproducible.\n",
        "\n",
        "---\n",
        "\n",
        "## **Random Forest vs. Single Decision Tree**\n",
        "\n",
        "| **Aspect**         | **Random Forest**                       | **Single Decision Tree**            |\n",
        "|--------------------|-----------------------------------------|-------------------------------------|\n",
        "| **Overfitting**    | Less prone to overfitting due to averaging | High risk of overfitting if not pruned |\n",
        "| **Performance**    | Higher accuracy due to aggregation of many trees | Prone to high variance and lower accuracy |\n",
        "| **Interpretability**| Harder to interpret (black-box)         | Easy to interpret (rule-based)     |\n",
        "| **Training Time**  | Slower due to many trees                | Faster as it‚Äôs just one tree       |\n",
        "| **Robustness**     | More robust to noise and outliers       | Sensitive to noise and outliers    |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Random Forest Classifier?**\n",
        "\n",
        "‚úÖ When you want a **high-accuracy model** without heavy tuning.  \n",
        "‚úÖ When your data has **multiple features** and you need a **model that reduces overfitting**.  \n",
        "‚úÖ When you need to **understand feature importance** in your dataset.  \n",
        "‚úÖ When you have **non-linear relationships** in the data that are difficult for simpler models to capture.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Hrp_CXctdywN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What are the main types of ensemble techniques?"
      ],
      "metadata": {
        "id": "YVPt0vPoeKYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Main Types of Ensemble Techniques üß©**\n",
        "\n",
        "Ensemble techniques are broadly classified into three main types based on how they combine the predictions of multiple models. Each type has its strengths and works best under different circumstances.\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Bagging (Bootstrap Aggregating)**  \n",
        "- **Idea**: Bagging combines multiple **independent models** trained on different subsets of the training data and then **aggregates** their predictions (via **majority voting** for classification or **averaging** for regression).  \n",
        "- **Goal**: **Reduce variance** by training multiple models on different random samples of the data.\n",
        "  \n",
        "#### **How it Works:**\n",
        "- Create **multiple bootstrap samples** (random samples with replacement) from the training dataset.\n",
        "- Train a model (typically a **weak learner** like a **decision tree**) on each bootstrap sample.\n",
        "- Make predictions by **aggregating** the predictions from all models (majority voting for classification or averaging for regression).\n",
        "\n",
        "#### **Example Models:**\n",
        "- **Random Forest** (multiple decision trees)\n",
        "- **Bagging Classifier** (using classifiers like decision trees or k-NN)\n",
        "- **Bagging Regressor** (using regression models like decision trees)\n",
        "\n",
        "#### **Advantages:**\n",
        "- Reduces **overfitting** (especially for high-variance models like decision trees).\n",
        "- Can **parallelize** easily since models are trained independently.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Boosting**  \n",
        "- **Idea**: Boosting combines **weak learners** sequentially, where each new model tries to **correct the errors** made by the previous one. The final prediction is a **weighted combination** of all individual models.\n",
        "- **Goal**: **Reduce bias** by focusing on difficult-to-classify examples and iteratively improving the model.\n",
        "\n",
        "#### **How it Works:**\n",
        "- Train an initial model.\n",
        "- In each subsequent round, assign more **weight** to the data points that were misclassified by the previous model.\n",
        "- Combine the predictions from all models by giving higher weight to better-performing models.\n",
        "\n",
        "#### **Example Models:**\n",
        "- **AdaBoost** (Adaptive Boosting)\n",
        "- **Gradient Boosting** (including **XGBoost**, **LightGBM**, **CatBoost**)\n",
        "- **LogitBoost**\n",
        "\n",
        "#### **Advantages:**\n",
        "- **Highly accurate**, often leading to the best performance.\n",
        "- Focuses on **misclassified points**, making it strong in capturing complex patterns in the data.\n",
        "- **Versatile**‚Äîcan be used for both classification and regression tasks.\n",
        "\n",
        "#### **Challenges:**\n",
        "- **Sensitive to noisy data** and outliers.\n",
        "- **Slow training time**, especially when using complex models.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Stacking (Stacked Generalization)**  \n",
        "- **Idea**: Stacking combines predictions from multiple models (often of different types) and then uses a **meta-model** (usually a simpler model) to learn how best to combine these predictions.  \n",
        "- **Goal**: Leverage the strengths of various types of models and **combine them optimally** to improve performance.\n",
        "\n",
        "#### **How it Works:**\n",
        "- Train multiple different models on the same dataset (e.g., decision trees, SVMs, logistic regression).\n",
        "- Use these models to make predictions on the training set.\n",
        "- Feed the predictions from these base models into a **meta-model** (like logistic regression or another classifier) to combine their outputs.\n",
        "\n",
        "#### **Example Models:**\n",
        "- **Logistic Regression** on top of multiple base models (e.g., decision trees, k-NN, SVMs)\n",
        "- **Neural Networks** used as meta-models\n",
        "- **Linear Model** used for combining the predictions of base models\n",
        "\n",
        "#### **Advantages:**\n",
        "- **Flexibility**‚Äîworks with diverse base models.\n",
        "- **Higher accuracy** compared to individual models since it combines the **best predictions** of each base model.\n",
        "- **Can use a variety of base learners**, including non-tree-based models.\n",
        "\n",
        "#### **Challenges:**\n",
        "- Can be **computationally expensive** as it requires training multiple models.\n",
        "- The meta-model might **overfit** if not carefully tuned.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Voting**  \n",
        "- **Idea**: Voting is the simplest ensemble method where predictions from multiple models are combined by either **majority voting** (for classification) or **averaging** (for regression).\n",
        "- **Goal**: Leverage multiple models to improve prediction accuracy without complex transformations.\n",
        "\n",
        "#### **How it Works:**\n",
        "- Train different models on the same dataset (e.g., decision trees, logistic regression, SVM).\n",
        "- For classification, use **majority voting** to determine the final class (the class that receives the most votes from individual models).\n",
        "- For regression, use the **average** of predictions from each model.\n",
        "\n",
        "#### **Example Models:**\n",
        "- **Voting Classifier** (combines different classifiers like Decision Tree, SVM, and Logistic Regression)\n",
        "- **Voting Regressor** (combines regression models)\n",
        "\n",
        "#### **Advantages:**\n",
        "- **Simple to implement**.\n",
        "- Can combine models of **different types** (heterogeneous models) for better diversity.\n",
        "- **Fast** compared to other ensemble techniques.\n",
        "\n",
        "#### **Challenges:**\n",
        "- Performance may not be as high as **boosting** or **stacking** methods.\n",
        "- Does not focus on correcting mistakes, so may be less accurate than boosting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Ensemble Techniques**\n",
        "\n",
        "| **Ensemble Method**   | **Type of Models**             | **Combines Predictions**        | **Focus**               | **Advantages**                        | **Challenges**                     |\n",
        "|-----------------------|--------------------------------|---------------------------------|-------------------------|---------------------------------------|------------------------------------|\n",
        "| **Bagging**           | Same type of models (e.g., trees) | Aggregating (voting/averaging)  | Reduces variance        | Reduces overfitting, simple to parallelize | Less effective for high-bias problems |\n",
        "| **Boosting**          | Same type of models (e.g., trees) | Weighted combination of models  | Reduces bias            | Very accurate, focuses on difficult cases | Prone to overfitting, slow training |\n",
        "| **Stacking**          | Different types of models       | Meta-model learns optimal combination | Leverages multiple models' strengths | Highly accurate, versatile          | Complex, computationally expensive |\n",
        "| **Voting**            | Different types of models       | Majority voting/averaging       | Combines diverse models | Simple, fast, and easy to implement   | May not perform as well as boosting |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Each Technique?**\n",
        "\n",
        "- **Bagging** (e.g., Random Forest) is great when you have a **high-variance model** and want to reduce overfitting.\n",
        "- **Boosting** (e.g., AdaBoost, XGBoost) works well when you need **high accuracy** and the model is **underfitting**.\n",
        "- **Stacking** is ideal when you want to combine the **strengths of different models** and can afford the computational cost.\n",
        "- **Voting** is useful when you need a **simple ensemble** and have multiple diverse models available.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3VA40wvxePue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.What is ensemble learning in machine learning?"
      ],
      "metadata": {
        "id": "6x7yMVfrekMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Ensemble Learning in Machine Learning** üåê\n",
        "\n",
        "**Ensemble learning** is a technique in machine learning where multiple **models** (often called **weak learners**) are combined to **solve a problem**. The main idea is that **combining multiple models** (even if they are weak individually) can produce a **stronger, more accurate model**. This technique improves the **predictive performance**, reduces the risk of **overfitting**, and often results in **better generalization** compared to individual models.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why Use Ensemble Learning?**\n",
        "\n",
        "1Ô∏è‚É£ **Better Performance**  \n",
        "   - Combining multiple models allows you to **correct the mistakes** of individual models. This results in higher **accuracy** and **reliability**.\n",
        "\n",
        "2Ô∏è‚É£ **Reduces Overfitting**  \n",
        "   - Individual models may overfit the training data. By aggregating multiple models, ensemble learning reduces the overall risk of overfitting and improves **generalization** to unseen data.\n",
        "\n",
        "3Ô∏è‚É£ **Increases Robustness**  \n",
        "   - Ensembles are more robust to noise and outliers because they consider predictions from several models instead of just one, minimizing the impact of any incorrect predictions from a single model.\n",
        "\n",
        "4Ô∏è‚É£ **Mitigates Bias and Variance**  \n",
        "   - Ensemble learning allows for a balance between **bias** (error due to overly simplistic models) and **variance** (error due to overly complex models). Depending on the ensemble technique used, you can target a reduction in either bias or variance.\n",
        "\n",
        "---\n",
        "\n",
        "## **Types of Ensemble Learning Methods**\n",
        "\n",
        "There are three main types of ensemble learning methods:\n",
        "\n",
        "### **1Ô∏è‚É£ Bagging (Bootstrap Aggregating)**\n",
        "   - **Idea**: Multiple models are trained independently on **different random subsets** of the training data, and their predictions are combined (usually by **voting** for classification or **averaging** for regression).\n",
        "   - **Goal**: **Reduce variance** by combining the predictions of many models.\n",
        "   - **Example**: **Random Forest**, where multiple decision trees are trained on different bootstrapped datasets.\n",
        "\n",
        "### **2Ô∏è‚É£ Boosting**\n",
        "   - **Idea**: Models are trained **sequentially**, and each new model focuses on the **errors** made by the previous models. The final prediction is a **weighted combination** of all individual models.\n",
        "   - **Goal**: **Reduce bias** by iteratively correcting the mistakes made by earlier models.\n",
        "   - **Example**: **AdaBoost**, **Gradient Boosting**, and **XGBoost**, where models like decision trees are trained to focus on hard-to-classify examples.\n",
        "\n",
        "### **3Ô∏è‚É£ Stacking (Stacked Generalization)**\n",
        "   - **Idea**: Multiple different models are trained on the same data, and then their predictions are combined by a **meta-model**. The meta-model learns how to optimally combine the predictions of the base models.\n",
        "   - **Goal**: Leverage the strengths of different models by combining them in an intelligent way.\n",
        "   - **Example**: Using **decision trees**, **SVMs**, and **logistic regression** as base models, and using a **logistic regression model** as the meta-model to combine their predictions.\n",
        "\n",
        "### **4Ô∏è‚É£ Voting**\n",
        "   - **Idea**: Multiple models are trained, and their predictions are combined by **majority voting** (classification) or **averaging** (regression). This is the simplest form of ensemble learning.\n",
        "   - **Goal**: Combine the predictions of several models for improved accuracy.\n",
        "   - **Example**: A **Voting Classifier** that combines predictions from decision trees, logistic regression, and SVM.\n",
        "\n",
        "---\n",
        "\n",
        "## **How Does Ensemble Learning Work?**\n",
        "\n",
        "### **Steps in an Ensemble Learning Approach:**\n",
        "1. **Train multiple models**:\n",
        "   - Depending on the method (bagging, boosting, stacking, or voting), train several models on the dataset.\n",
        "2. **Combine their predictions**:\n",
        "   - After training, combine the predictions from each model. In bagging, it's usually **averaging** or **voting**, while in boosting and stacking, it's a **weighted combination** or **meta-model aggregation**.\n",
        "3. **Final Prediction**:\n",
        "   - The final output is made based on the combined predictions of all models.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages of Ensemble Learning** üåü\n",
        "\n",
        "1. **Improved Accuracy**  \n",
        "   - The most significant advantage is improved performance compared to individual models. By combining models, you reduce the likelihood of **errors** and improve generalization.\n",
        "\n",
        "2. **Robustness to Overfitting**  \n",
        "   - Especially with techniques like bagging, ensemble methods reduce the **variance** of a model, which leads to better performance on unseen data.\n",
        "\n",
        "3. **Better Generalization**  \n",
        "   - Because different models capture different aspects of the data, the ensemble tends to generalize better than any single model, especially when the individual models make errors in different areas.\n",
        "\n",
        "4. **Error Reduction**  \n",
        "   - Errors that may occur from an individual model are less likely to affect the overall ensemble model significantly, as it considers predictions from multiple models.\n",
        "\n",
        "---\n",
        "\n",
        "## **Challenges of Ensemble Learning** ‚ö†Ô∏è\n",
        "\n",
        "1. **Computational Complexity**  \n",
        "   - Training and maintaining multiple models can be **computationally expensive**, especially for large datasets. This requires more memory and processing power.\n",
        "\n",
        "2. **Interpretability**  \n",
        "   - Ensemble methods, especially **boosting** and **stacking**, can be hard to interpret due to the complexity of combining many models, making them **black-box** approaches.\n",
        "\n",
        "3. **Risk of Overfitting (in Boosting)**  \n",
        "   - While boosting generally reduces bias, it can sometimes **overfit** the data, especially if the model is too complex or if there is too much noise in the data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Popular Ensemble Learning Algorithms**\n",
        "\n",
        "1. **Random Forest** (Bagging)  \n",
        "   - Combines decision trees trained on different subsets of the data, reducing overfitting and improving prediction accuracy.\n",
        "\n",
        "2. **AdaBoost** (Boosting)  \n",
        "   - Focuses on misclassified instances by adjusting their weights, making it robust against errors.\n",
        "\n",
        "3. **Gradient Boosting** (Boosting)  \n",
        "   - Builds trees sequentially, each trying to correct the mistakes of the previous one. It is widely used for tasks where high accuracy is required.\n",
        "\n",
        "4. **XGBoost** (Boosting)  \n",
        "   - A highly optimized version of gradient boosting that is faster and more efficient. XGBoost is one of the most popular ensemble algorithms for Kaggle competitions.\n",
        "\n",
        "5. **LightGBM** (Boosting)  \n",
        "   - A variant of gradient boosting designed for faster training and efficiency, particularly on large datasets.\n",
        "\n",
        "6. **Stacking**  \n",
        "   - Combines multiple models (often heterogeneous) by learning the best way to combine them through a meta-model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary: Ensemble Learning in a Nutshell**\n",
        "\n",
        "- **Ensemble learning** improves the performance of machine learning models by combining multiple weak learners to create a stronger learner.\n",
        "- **Bagging**, **Boosting**, **Stacking**, and **Voting** are the most common types of ensemble techniques.\n",
        "- It helps in improving **accuracy**, reducing **overfitting**, and providing **robustness**.\n",
        "- The main challenge is the **increased computational cost** and the **complexity of interpreting results**."
      ],
      "metadata": {
        "id": "cpkpqlQbeqnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.When should we avoid using ensemble methods?"
      ],
      "metadata": {
        "id": "k2Yy0s8Te-OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **When to Avoid Using Ensemble Methods ‚ö†Ô∏è**\n",
        "\n",
        "While **ensemble methods** can significantly improve the performance of machine learning models, there are certain situations where they may not be the best choice. Here are some **scenarios** where you might want to avoid or reconsider using ensemble techniques:\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Small Datasets**  \n",
        "Ensemble methods typically perform best with **larger datasets** because they combine predictions from multiple models to improve accuracy. On small datasets, the potential benefits of ensemble methods can be limited or even detrimental.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - With limited data, each individual model might not have enough variety in its training set to learn meaningful patterns.  \n",
        "  - **Ensemble models** may simply end up overfitting the data, especially when using models that are prone to overfitting like decision trees.\n",
        "  \n",
        "- **Alternatives**:  \n",
        "  - Use simpler, less complex models (like **Logistic Regression** or **k-NN**) which are more likely to perform well on small datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Interpretability is Crucial**  \n",
        "Ensemble methods, especially techniques like **Random Forest** and **Boosting**, tend to act as **black-box models**, making them **difficult to interpret**.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - If your application requires a model that is easy to understand and explain (for example, in medical or financial domains where **transparency** is important), ensemble methods may not be ideal because they combine many models and are harder to interpret.\n",
        "  \n",
        "- **Alternatives**:  \n",
        "  - Consider simpler models like **decision trees**, **logistic regression**, or **linear models**, which offer better **transparency**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ High Computational Resources**  \n",
        "Ensemble methods, especially **boosting** (like **XGBoost**, **LightGBM**) and **stacking**, require training **multiple models**, which can be computationally expensive, especially with large datasets.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - If you're working with limited computational resources (e.g., limited processing power, memory), training and tuning ensemble methods can become time-consuming and may exceed your available resources.\n",
        "  \n",
        "- **Alternatives**:  \n",
        "  - **Single model** approaches or simpler algorithms that require less computation, such as **Logistic Regression**, **Naive Bayes**, or a single **decision tree**, might be more feasible.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Simple Problems**  \n",
        "Ensemble methods are most beneficial when the problem is complex and the base models are weak (i.e., have high bias or variance). If you're dealing with a **simple problem** where a single model can achieve good performance, an ensemble may not be necessary.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - Using ensemble methods on simple problems might lead to overcomplicated models that do not improve much over a simpler, single model.  \n",
        "  - The additional complexity of ensemble methods could make the solution harder to tune, train, and maintain without a substantial improvement in performance.\n",
        "  \n",
        "- **Alternatives**:  \n",
        "  - Try simpler models like **Logistic Regression**, **Linear Regression**, or a **single decision tree**. If the model performs well, there‚Äôs no need to overcomplicate.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Noise and Outliers**  \n",
        "Some ensemble methods, especially **boosting**, are **sensitive to noisy data** or **outliers** because they focus heavily on correcting errors from previous models.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - If your dataset contains significant amounts of noise or outliers, the boosting process might end up **overfitting** to those noisy points, reducing generalization performance.  \n",
        "  - In such cases, the ensemble model may become too complex and fail to generalize well on unseen data.\n",
        "\n",
        "- **Alternatives**:  \n",
        "  - Consider using robust models like **Random Forests** (via bagging), which are less sensitive to outliers compared to boosting methods.\n",
        "  - Alternatively, **data preprocessing** techniques such as **outlier detection** and **cleaning** could help mitigate the issue.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ When Prediction Speed is Crucial**  \n",
        "Ensemble methods, especially those like **Random Forests** and **XGBoost**, require **multiple models** to be run in parallel or sequentially. This can slow down prediction times, which may be problematic in real-time systems where **fast predictions** are necessary.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - If your application requires **real-time** predictions or low-latency systems (e.g., **autonomous vehicles**, **fraud detection in real-time**), the prediction time of ensemble methods can become a bottleneck.\n",
        "  \n",
        "- **Alternatives**:  \n",
        "  - A **single model** like **Logistic Regression**, **Naive Bayes**, or a **single decision tree** can offer faster predictions since they require only one model to be evaluated at prediction time.\n",
        "\n",
        "---\n",
        "\n",
        "### **7Ô∏è‚É£ When You Have Already Optimized a Single Model**  \n",
        "If you've spent considerable time **tuning** a single model and achieved good performance, adding ensemble methods might not bring substantial improvements.\n",
        "\n",
        "- **Why Avoid**:  \n",
        "  - If a single model (e.g., **XGBoost**, **SVM**, **Neural Networks**) is already well-optimized and achieving high accuracy, ensembles might not offer significant gains and could just introduce unnecessary complexity.\n",
        "\n",
        "- **Alternatives**:  \n",
        "  - Stick with the **single model** and focus on optimizing it further, e.g., through **hyperparameter tuning**, or by experimenting with **feature engineering**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Summary: When to Avoid Ensemble Methods**\n",
        "\n",
        "1. **Small Datasets**: Risk of overfitting due to limited data.\n",
        "2. **Need for Interpretability**: If the problem demands an easily interpretable model.\n",
        "3. **Limited Computational Resources**: Ensembles are computationally expensive, especially boosting and stacking.\n",
        "4. **Simple Problems**: No need to overcomplicate with an ensemble if a single model works well.\n",
        "5. **Noise and Outliers**: Some ensemble methods, like boosting, are sensitive to noise and outliers.\n",
        "6. **Real-Time Prediction Requirements**: Ensemble methods can be slow for fast predictions.\n",
        "7. **Already Optimized Single Model**: When you have a well-tuned model that performs well on its own.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "lT_ZM7GvfDBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. How does Bagging help in reducing overfitting?"
      ],
      "metadata": {
        "id": "Wf9z3TWefg51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **How Bagging Helps in Reducing Overfitting** üõ°Ô∏è\n",
        "\n",
        "**Bagging** (Bootstrap Aggregating) is an ensemble learning technique that can help reduce **overfitting** by combining the predictions of multiple models. Here's how it works and why it helps to minimize overfitting:\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Training on Different Subsets of Data (Bootstrap Sampling)**\n",
        "\n",
        "- **How it works**:  \n",
        "  - In Bagging, multiple **bootstrap samples** (random samples with replacement) are created from the original training dataset. Each of these samples is used to train a separate **model**.\n",
        "  - Since each model is trained on a slightly different subset of data, it reduces the likelihood that any one model will **overfit** the training data. The models focus on different parts of the data and capture different patterns.\n",
        "\n",
        "- **How it helps reduce overfitting**:  \n",
        "  - **Overfitting** happens when a model captures too much noise or irrelevant details in the training data. If a model is overly complex, it may **fit** to outliers or small fluctuations in the training data.\n",
        "  - Since **each individual model** in Bagging is trained on a different subset of the data, no single model can overfit the entire dataset. This introduces **diversity** in the models, which helps in **reducing overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Aggregation of Predictions (Averaging or Majority Voting)**\n",
        "\n",
        "- **How it works**:  \n",
        "  - After training multiple models, Bagging **combines** their predictions to make the final prediction. For classification tasks, it typically uses **majority voting**, where the most common prediction across all models is selected. For regression tasks, it uses the **average** of the predictions from all models.\n",
        "  \n",
        "- **How it helps reduce overfitting**:  \n",
        "  - When you **combine multiple models**, the individual errors (including overfitting errors) of each model are less likely to have a major impact on the final prediction. By averaging out the predictions or using voting, Bagging smooths out the effect of any individual model that might have overfitted to certain parts of the data.\n",
        "  - For example, if one model overfits to some noise in the data, it will likely make wrong predictions. However, the other models, trained on different subsets of the data, will not overfit in the same way, and their predictions will \"correct\" the overfit model‚Äôs predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Reduces Model Variance**\n",
        "\n",
        "- **How it works**:  \n",
        "  - Bagging focuses on reducing the **variance** of a model by averaging out the predictions of multiple models trained on different subsets of the data.\n",
        "  \n",
        "- **How it helps reduce overfitting**:  \n",
        "  - A model with high variance can overfit the training data, meaning it will perform well on the training set but poorly on unseen data. By combining multiple models, each with slightly different training data, Bagging **reduces the overall variance** of the final model.\n",
        "  - For instance, a single **decision tree** might have high variance and overfit, but by averaging many decision trees trained on different samples of the data, the overall model becomes **less sensitive** to small variations and noise in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Independent Models Reduce the Impact of Individual Overfitting**\n",
        "\n",
        "- **How it works**:  \n",
        "  - Bagging works by training multiple models **independently**. Each model is exposed to different parts of the data, and their predictions are combined afterward.\n",
        "  \n",
        "- **How it helps reduce overfitting**:  \n",
        "  - If one model overfits the data, it is unlikely to affect the overall prediction much because it is combined with the predictions of other independent models.  \n",
        "  - This **reduces the risk of overfitting** as the final prediction is not overly reliant on any single model, but rather an average (or majority) from all models.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Works Well with High-Variance Models**\n",
        "\n",
        "- **How it works**:  \n",
        "  - Bagging is especially useful for **high-variance models**, such as **decision trees**, which are prone to overfitting. Decision trees can create overly complex models that capture noise and minor fluctuations in the data.\n",
        "  \n",
        "- **How it helps reduce overfitting**:  \n",
        "  - When we apply Bagging to decision trees (as in **Random Forests**), we end up with an ensemble of trees that each capture different aspects of the data. Since Bagging averages the predictions, the final ensemble model is **more robust** and less prone to overfitting than individual decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "### **In Summary: How Bagging Helps in Reducing Overfitting**\n",
        "\n",
        "1. **Training on different data subsets**:  \n",
        "   Bagging trains multiple models on different subsets of the data, which reduces the chance of overfitting to any one particular subset or noise in the data.\n",
        "\n",
        "2. **Aggregating predictions**:  \n",
        "   By aggregating the predictions from multiple models (via majority voting or averaging), Bagging reduces the impact of any individual model that may have overfitted to the training data.\n",
        "\n",
        "3. **Reduction in variance**:  \n",
        "   Bagging reduces model **variance** by combining models, leading to improved generalization and reduced overfitting.\n",
        "\n",
        "4. **Independence of models**:  \n",
        "   Since each model is trained independently on different data, individual overfitting does not heavily affect the final ensemble prediction.\n",
        "\n",
        "5. **Great for high-variance models**:  \n",
        "   Bagging is particularly effective with high-variance models (like decision trees), where overfitting is a concern. The ensemble of models helps to create a more stable and accurate prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**:  \n",
        "Bagging helps to reduce overfitting by creating an ensemble of models that are trained on different data subsets and then combined to make the final prediction. This process smooths out individual model errors, reduces variance, and leads to a more generalizable model, particularly for high-variance learners like decision trees.\n"
      ],
      "metadata": {
        "id": "ZKZw3jXzfnsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Why is Random Forest better than a single Decision Tree?"
      ],
      "metadata": {
        "id": "fO4NvoZLf57r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Why Random Forest is Better Than a Single Decision Tree** üå≥\n",
        "\n",
        "**Random Forest** is an ensemble learning method that combines multiple **decision trees** to improve overall performance. A single **decision tree** can be quite powerful, but it has its limitations, which is why **Random Forest** generally outperforms it. Let's explore the key reasons why **Random Forest** is often better than a single decision tree:\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Reduces Overfitting**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A decision tree can **overfit** the training data, especially when the tree is **deep** and complex. It may fit the training data perfectly, capturing noise and outliers, which reduces its ability to generalize to unseen data.\n",
        "  \n",
        "- **Random Forest**:  \n",
        "  In contrast, **Random Forest** reduces overfitting by **aggregating the predictions** from multiple trees. Each tree in a Random Forest is trained on a **random subset** of the data, and the final prediction is made by averaging the predictions (for regression) or using **majority voting** (for classification). This reduces the variance of individual trees, making the overall model less likely to overfit.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest averages out the errors of individual trees, making it less prone to overfitting compared to a single decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Improved Generalization**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A single decision tree tends to **memorize** the training data rather than generalizing well to unseen data. This is particularly true if the tree is allowed to grow too deep.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Since **Random Forest** aggregates multiple trees that are trained on different random subsets of data, it helps the model **generalize** better by learning from various parts of the data. The ensemble of trees tends to focus on the **overall patterns** rather than specific details in the training data.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  The combination of many trees with diverse training data improves the model‚Äôs ability to generalize to new, unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Reduces Variance**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A single decision tree can have **high variance**, meaning its performance can drastically change with small changes in the training data. This is because it can easily overfit the training set and capture noise, leading to high sensitivity to small fluctuations in the data.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Random Forest reduces **variance** by training multiple trees on different subsets of the data. Each tree in the forest makes slightly different errors, and when combined, these errors tend to cancel each other out. This results in a model with **lower variance** and better overall performance on unseen data.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  The ensemble approach makes Random Forest more **robust** and **stable** compared to a single decision tree, especially when dealing with noisy data.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ More Robust to Noise and Outliers**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A decision tree can easily be influenced by outliers or noisy data points. If an outlier appears in the training set, the tree might create a complex decision rule that fits that outlier, leading to poor generalization.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Since Random Forest aggregates the predictions from multiple trees, the effect of any outlier or noise is **diluted**. The trees in the forest may focus on different parts of the data, and the influence of any individual noisy data point is minimized in the final prediction.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest is less sensitive to outliers and noise compared to a single decision tree, making it more **robust**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Handles Missing Data Better**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  When training a single decision tree, **missing values** can be problematic. A decision tree may struggle with missing values, leading to biased splits or requiring imputation strategies.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Random Forest handles missing values more effectively. If a tree is trained on a subset of the data with missing values, it can still make good predictions because it aggregates results from multiple trees, each trained with a different subset of data. Some trees may be trained without certain missing data points, so the final result will still be reliable.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest is more **flexible** in handling missing data without requiring extensive data preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ Can Handle Complex Relationships**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A single decision tree may struggle to capture complex relationships between features, as it splits data based on one feature at a time. This can lead to overly simplistic models that miss important interactions between features.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Since Random Forest combines multiple decision trees, it is better at capturing complex relationships. The different trees in the forest may split data on different features, allowing the model to capture **non-linear** relationships and **interactions** between features more effectively.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest is more capable of capturing **complex interactions** between features than a single decision tree.\n",
        "\n",
        "---\n",
        "\n",
        "### **7Ô∏è‚É£ Feature Importance Estimation**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A single decision tree can provide insights into which features are important for making predictions, but it can be biased if certain features dominate the splits or the tree is too deep.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  Random Forest naturally provides a more **accurate and robust** estimation of feature importance. By aggregating results from multiple trees, Random Forest can give a more reliable assessment of which features are truly important for prediction.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest provides better feature importance estimation because it considers a variety of trees and subsets of data, leading to less bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **8Ô∏è‚É£ Better Accuracy and Reliability**\n",
        "\n",
        "- **Single Decision Tree**:  \n",
        "  A single decision tree might give a decent performance on the training data but is often prone to large fluctuations in its predictions on unseen data, leading to unreliable results.\n",
        "\n",
        "- **Random Forest**:  \n",
        "  By averaging out the predictions of many decision trees, Random Forest is more **accurate** and **reliable**. The final predictions benefit from the **wisdom of the crowd**, where multiple models contribute to the final decision.\n",
        "\n",
        "- **Key Advantage**:  \n",
        "  Random Forest generally provides **higher accuracy** and **more reliable** predictions than a single decision tree, especially on complex datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary: Why Random Forest is Better Than a Single Decision Tree**\n",
        "\n",
        "1. **Reduces Overfitting**: By training multiple trees on different subsets of data and aggregating their predictions, Random Forest reduces the likelihood of overfitting that is common in a single decision tree.\n",
        "2. **Improved Generalization**: The ensemble of trees learns a more general pattern, improving performance on unseen data.\n",
        "3. **Reduces Variance**: Multiple trees help reduce the variance of the model, making Random Forest more stable and less sensitive to fluctuations in the data.\n",
        "4. **More Robust to Noise**: Random Forest is less sensitive to outliers and noisy data compared to a single decision tree.\n",
        "5. **Handles Missing Data Better**: Random Forest can handle missing data more effectively without requiring extensive preprocessing.\n",
        "6. **Captures Complex Relationships**: Random Forest can model complex relationships and interactions between features that a single decision tree may miss.\n",
        "7. **Better Feature Importance Estimation**: Random Forest provides a more robust estimate of feature importance compared to a single tree.\n",
        "8. **Higher Accuracy and Reliability**: Overall, Random Forest is typically more accurate and reliable due to its ensemble nature.\n",
        "\n",
        "---\n",
        "\n",
        "In short, **Random Forest** combines the strengths of multiple decision trees to produce a model that is **more accurate**, **robust**, and **reliable** than a single decision tree, especially in complex and noisy data scenarios.\n"
      ],
      "metadata": {
        "id": "CpHNnUa6f_d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.What is the role of bootstrap sampling in Bagging?"
      ],
      "metadata": {
        "id": "uPHuUF68gUyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Role of Bootstrap Sampling in Bagging** üõ†Ô∏è\n",
        "\n",
        "**Bootstrap sampling** plays a critical role in the **Bagging (Bootstrap Aggregating)** method, which is an ensemble technique designed to improve the performance of machine learning models. Here's a breakdown of how **bootstrap sampling** works and why it's so important in **Bagging**:\n",
        "\n",
        "---\n",
        "\n",
        "### **What is Bootstrap Sampling?**\n",
        "\n",
        "- **Bootstrap Sampling** refers to the process of **randomly selecting samples** from the original dataset, but with **replacement**. This means that some data points from the original dataset may be selected multiple times, while others may not be selected at all.\n",
        "- The size of each **bootstrap sample** is usually the same as the original dataset.\n",
        "\n",
        "In the context of **Bagging**, **multiple bootstrap samples** are generated, and **each model** in the ensemble is trained on one of these samples.\n",
        "\n",
        "---\n",
        "\n",
        "### **Role of Bootstrap Sampling in Bagging**\n",
        "\n",
        "1. **Creates Diversity Among Models** üå±\n",
        "   - Each model in Bagging is trained on a **different subset** of the data, thanks to the **randomness** introduced by bootstrap sampling. Even though the data is drawn from the same original dataset, because the samples are random (with replacement), each model will receive slightly different training data. This diversity helps to create **weak learners** (models that might individually not perform well), which when aggregated, can produce a strong, generalized model.\n",
        "   \n",
        "2. **Reduces Overfitting by Lowering Variance** üõ°Ô∏è\n",
        "   - One of the key strengths of Bagging is its ability to **reduce variance** in models like decision trees that are prone to **overfitting**. Bootstrap sampling helps ensure that the **training data for each tree** is different, meaning each tree learns different aspects of the data. When Bagging aggregates these trees' predictions (either by voting for classification or averaging for regression), the model's overall **variance** is reduced, making it less likely to overfit the training data. Essentially, the errors of individual models \"cancel out\" when combined, leading to a more **stable and accurate** prediction.\n",
        "\n",
        "3. **Improves Generalization** üìà\n",
        "   - Since each model in Bagging is trained on a different subset of data, the overall ensemble model benefits from having learned from various aspects of the data. **Generalization** refers to the model's ability to perform well on unseen data. By using **bootstrap sampling**, Bagging creates an ensemble that is better at generalizing than a single model, as the ensemble models are less likely to memorize the training data or focus too heavily on particular patterns or noise.\n",
        "   \n",
        "4. **Enables Model Averaging** üßÆ\n",
        "   - The **aggregation** of predictions from multiple models (trees) is an essential step in Bagging. Because the models are trained on different samples of the data (through bootstrap sampling), they are likely to make **slightly different predictions**. By averaging the predictions for regression or using **majority voting** for classification, Bagging reduces the impact of errors or outliers from any one individual model, leading to more accurate overall predictions.\n",
        "\n",
        "5. **Efficient Use of Data** üßë‚Äçüíª\n",
        "   - Since bootstrap sampling is done with **replacement**, each model can learn from the same set of training data multiple times (some data points may be used in more than one tree). This allows the model to make better use of the data, while still preserving diversity. In a sense, the model is able to see many \"views\" of the data, leading to a stronger generalization ability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of Bootstrap Sampling in Bagging**\n",
        "\n",
        "Suppose you have a dataset with 1000 data points:\n",
        "\n",
        "- **Bootstrap Sample 1** might contain data points: [1, 5, 3, 7, 1, 8, 9, ...]  \n",
        "  (Notice that some points are repeated and others may be missing.)\n",
        "\n",
        "- **Bootstrap Sample 2** might contain data points: [2, 4, 7, 2, 6, 8, ...]  \n",
        "  (Again, data points may repeat, and some data points from the original dataset may not appear at all.)\n",
        "\n",
        "- **Bootstrap Sample 3** might contain data points: [3, 1, 5, 6, 9, 2, ...]  \n",
        "  (A different random subset with possible repetitions and omissions.)\n",
        "\n",
        "Each of these subsets is used to train a separate model, and once all models are trained, their **predictions** are combined. For **classification tasks**, Bagging uses **majority voting**, and for **regression tasks**, it takes the **average** of all model predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Benefits of Bootstrap Sampling in Bagging**\n",
        "\n",
        "1. **Reduces Variance**: By training each model on a slightly different subset of the data, Bagging reduces the variance in predictions, helping to prevent overfitting.\n",
        "2. **Improves Accuracy**: The aggregation of models trained on random subsets allows the ensemble to make more accurate and reliable predictions.\n",
        "3. **Leverages the Power of Weak Learners**: Bootstrap sampling helps in creating multiple weak models, which when combined, form a powerful, well-generalized model.\n",
        "4. **Efficiency in Learning**: With bootstrap sampling, every model gets to learn from multiple copies of the data, making efficient use of the dataset without needing to split the data into distinct subsets (as in cross-validation).\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Bootstrap Sampling in Bagging**\n",
        "\n",
        "- **Boosts Model Diversity**: Different random subsets lead to different models.\n",
        "- **Reduces Overfitting**: By averaging or voting, errors cancel out, and the overall variance is lowered.\n",
        "- **Improves Generalization**: The ensemble learns multiple views of the data, helping it generalize better.\n",
        "- **Efficient**: Models use the data efficiently through random resampling.\n",
        "\n",
        "---\n",
        "\n",
        "In essence, **bootstrap sampling** is crucial in **Bagging** because it allows the method to create **diverse models** from the same data, which, when aggregated, leads to **reduced variance**, **improved accuracy**, and **better generalization**.\n"
      ],
      "metadata": {
        "id": "X3IFwXyjgZqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.What are some real-world applications of ensemble techniques?"
      ],
      "metadata": {
        "id": "le3Z_6xtgsdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Real-World Applications of Ensemble Techniques** üåç\n",
        "\n",
        "Ensemble techniques are widely used in many domains because they combine the strengths of multiple models to improve accuracy, robustness, and generalization. Below are some common real-world applications of ensemble methods across different industries:\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Finance and Stock Market Prediction üìâüìà**\n",
        "\n",
        "- **Problem**: Predicting stock prices, stock market trends, or identifying fraud is a complex task that requires high accuracy.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests**, **Gradient Boosting**, and **XGBoost** are often employed to predict stock prices or detect fraudulent transactions.\n",
        "  - Ensemble methods can aggregate predictions from multiple models to better capture patterns in financial data, reducing the risk of overfitting to specific market conditions.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Credit scoring**: Banks use ensemble methods to predict whether a person will default on a loan. By combining various decision trees or models, the ensemble can improve the accuracy of credit score predictions.\n",
        "  \n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Medical Diagnostics and Healthcare üè•**\n",
        "\n",
        "- **Problem**: Classifying diseases, predicting patient outcomes, and detecting medical conditions from complex datasets like medical images or genetic data require accurate predictions.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests** and **Gradient Boosting Machines (GBMs)** are frequently used in medical diagnostics.\n",
        "  - **Voting Classifiers** or **Stacked Ensembles** are used in image classification (e.g., detecting tumors from medical scans) by combining the predictions from different neural networks or other classifiers.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Breast cancer detection**: A Random Forest classifier can be trained on different features from mammogram images or medical records, and its output can be aggregated to improve detection accuracy.\n",
        "  - **Heart disease prediction**: Multiple models like decision trees, logistic regression, and neural networks can be used together to predict the likelihood of heart disease based on patient data, such as age, blood pressure, and cholesterol levels.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Natural Language Processing (NLP) and Sentiment Analysis üìù**\n",
        "\n",
        "- **Problem**: Sentiment analysis, text classification, and language translation often need multiple models to handle the complexity and ambiguity of human language.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests**, **XGBoost**, and **Voting Classifiers** are popular in NLP tasks.\n",
        "  - **Stacked Generalization (Stacking)** can combine multiple models such as logistic regression, decision trees, and neural networks to improve text classification performance.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Sentiment analysis**: Ensemble models like Random Forest or Gradient Boosting can aggregate predictions from different classifiers to predict whether the sentiment of a review is positive, negative, or neutral.\n",
        "  - **Spam email detection**: Multiple models can be combined to detect spam emails based on features like keywords, frequency of words, sender, and metadata.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Image and Video Processing üì∏**\n",
        "\n",
        "- **Problem**: Image classification, object detection, and facial recognition often require high accuracy, especially in complex real-world scenarios.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Bagging** (e.g., Random Forests) and **Boosting** (e.g., XGBoost, AdaBoost) are often used for image recognition and object detection tasks.\n",
        "  - **Stacked models** or **Voting classifiers** can aggregate the predictions of different convolutional neural networks (CNNs) or traditional machine learning models.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Object detection in images**: Ensemble methods can combine the predictions from different object detection models (like YOLO, Faster R-CNN, etc.) to improve detection accuracy.\n",
        "  - **Face recognition**: Random Forests or XGBoost classifiers can aggregate multiple model outputs to better classify faces, reducing the likelihood of errors due to variations in lighting or angle.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Autonomous Vehicles and Robotics üöóü§ñ**\n",
        "\n",
        "- **Problem**: Autonomous vehicles require complex decision-making systems that can process large amounts of sensor data (from cameras, lidar, GPS, etc.) in real-time.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forest**, **XGBoost**, and **Stacking** are used to combine predictions from multiple models that process different types of data (e.g., image, sensor, and GPS data).\n",
        "  - **Ensemble learning** can be applied to tasks like object detection, path planning, and obstacle avoidance in autonomous vehicles.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Obstacle detection and path planning**: Different models might be trained to detect different types of objects (pedestrians, vehicles, road signs). An ensemble approach can combine the predictions from all models to improve the accuracy of detecting and avoiding obstacles.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ E-commerce and Recommendation Systems üõçÔ∏è**\n",
        "\n",
        "- **Problem**: Providing personalized recommendations to users, such as product recommendations, movie recommendations, or personalized search results.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Gradient Boosting Machines (GBMs)**, **Random Forests**, and **Stacking** can be used to combine models that predict user preferences based on past behavior and demographic data.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Product recommendations**: By using an ensemble of models (e.g., collaborative filtering, content-based filtering, and regression models), e-commerce platforms can provide more accurate product recommendations to users based on their past activity and preferences.\n",
        "  - **Movie recommendation systems**: Ensemble techniques can combine collaborative filtering models (based on user-item interactions) and content-based models (based on movie genres, actors, etc.) to improve recommendation accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **7Ô∏è‚É£ Customer Churn Prediction üìä**\n",
        "\n",
        "- **Problem**: Predicting customer churn (the likelihood of a customer leaving a service) is critical for many businesses, such as telecommunications, banking, and subscription services.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests** and **Gradient Boosting** are commonly used to combine predictions from multiple models trained on historical customer data.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Telecom industry**: An ensemble of classifiers (e.g., logistic regression, decision trees, and neural networks) can predict the likelihood that a customer will churn based on their usage patterns, payment history, and customer service interactions.\n",
        "  - **Subscription services**: By combining multiple models, businesses can more accurately predict which customers are at risk of canceling their subscription and take preventative actions.\n",
        "\n",
        "---\n",
        "\n",
        "### **8Ô∏è‚É£ Marketing and Customer Segmentation üì£**\n",
        "\n",
        "- **Problem**: Identifying customer segments or predicting the effectiveness of marketing campaigns often requires high accuracy.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests** and **Gradient Boosting Machines** are often used to segment customers based on purchasing behavior, demographics, and other features.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Targeted marketing**: Ensemble methods can combine different classifiers to predict which customers are most likely to respond to a particular marketing campaign, allowing companies to target the right audience more effectively.\n",
        "  - **Customer segmentation**: By using ensemble methods, businesses can segment customers more accurately into groups (e.g., high-value, frequent shoppers) to tailor their marketing strategies.\n",
        "\n",
        "---\n",
        "\n",
        "### **9Ô∏è‚É£ Sports Analytics üèÄ‚öΩ**\n",
        "\n",
        "- **Problem**: Predicting outcomes of games, player performance, or injury risks involves complex, high-dimensional data.\n",
        "- **Ensemble Techniques Used**:\n",
        "  - **Random Forests** and **XGBoost** are popular for predicting outcomes, player statistics, and even scouting potential players based on performance metrics.\n",
        "  \n",
        "- **Example**:  \n",
        "  - **Game outcome prediction**: An ensemble of models can be used to predict the outcome of sports games by considering factors such as team performance, player statistics, weather, and historical data.\n",
        "  - **Player performance analysis**: Ensemble techniques can combine various models to predict player performance and potential, helping teams make informed decisions during recruitment.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion** üèÜ\n",
        "\n",
        "Ensemble techniques, such as **Bagging**, **Boosting**, and **Stacking**, are widely used across different industries and applications. By aggregating the outputs of multiple models, they improve **accuracy**, **robustness**, and **generalization**. Whether it's predicting stock market trends, diagnosing diseases, or enhancing recommendation systems, ensemble learning plays a crucial role in producing more reliable and accurate predictions.\n"
      ],
      "metadata": {
        "id": "1z71ieolgxJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "YwskeDYLhK04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.### **Difference Between Bagging and Boosting** üåü\n",
        "\n",
        "**Bagging** and **Boosting** are both popular ensemble learning techniques used to improve the performance of machine learning models, but they approach the task in fundamentally different ways. Below is a detailed comparison between **Bagging** and **Boosting**:\n",
        "\n",
        "---\n",
        "\n",
        "### **1Ô∏è‚É£ Approach to Combining Models**\n",
        "\n",
        "- **Bagging (Bootstrap Aggregating)**:\n",
        "  - **Independent models**: Bagging trains multiple models independently in parallel.\n",
        "  - It uses **random sampling with replacement** (bootstrap sampling) to create different training datasets for each model.\n",
        "  - The final prediction is made by **averaging** (for regression) or **majority voting** (for classification) the predictions from all the individual models.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - **Sequential models**: Boosting trains models **sequentially**, with each new model trying to correct the errors made by the previous models.\n",
        "  - It assigns more weight to the data points that are **misclassified** by the earlier models. In other words, it focuses on difficult-to-predict examples.\n",
        "  - The final prediction is made by **combining** the weighted predictions of all models (often using a weighted average or weighted voting).\n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Model Focus**\n",
        "\n",
        "- **Bagging**:\n",
        "  - Bagging focuses on **reducing variance** by averaging the predictions of several models trained on different subsets of the data.\n",
        "  - It works best when the individual models have high variance (e.g., **Decision Trees**) and is used to improve the model's stability and prevent overfitting.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting focuses on **reducing bias** by iteratively improving the performance of weak models.\n",
        "  - It is most effective when the model is underfitting (i.e., has high bias) and aims to convert weak models into a strong predictive model by correcting the errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Training Process**\n",
        "\n",
        "- **Bagging**:\n",
        "  - The training of models is done **in parallel**, meaning each model is trained independently on different subsets of the data.\n",
        "  - There is no dependency between the models, and each model is trained in isolation.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - The models are trained **sequentially**, with each model being dependent on the performance of the previous one.\n",
        "  - Each new model is focused on the **mistakes** made by the previous model and tries to correct those errors by giving more weight to the misclassified data points.\n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Data Sampling**\n",
        "\n",
        "- **Bagging**:\n",
        "  - Each model is trained on a **random subset** of the training data, created by **sampling with replacement** (bootstrap sampling).\n",
        "  - This means that some data points may appear multiple times in a single model's training data, while others might not be selected at all.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting uses **all the training data** for each model, but it **adjusts the weights** of the data points based on whether they were correctly or incorrectly classified by previous models.\n",
        "  - Misclassified points get higher weights, forcing the model to focus on these harder-to-predict cases.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Parallel vs Sequential**\n",
        "\n",
        "- **Bagging**:\n",
        "  - Bagging allows for **parallel training** of the models since each model is independent.\n",
        "  - This makes Bagging faster to train when the models are computationally expensive (e.g., decision trees or other complex models).\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting requires **sequential training** of models, which can make it slower than Bagging, especially for large datasets or models.\n",
        "  - Each model builds upon the previous one, so training is done one after another.\n",
        "\n",
        "---\n",
        "\n",
        "### **6Ô∏è‚É£ Model Weighting in Final Prediction**\n",
        "\n",
        "- **Bagging**:\n",
        "  - **Equal weighting**: All individual models in Bagging contribute equally to the final prediction. Whether a model is correct or not, its output is treated with equal importance.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - **Weighted models**: In Boosting, models that perform well are given more weight in the final prediction, while models that perform poorly (i.e., those that focus on harder cases) are given less weight.\n",
        "\n",
        "---\n",
        "\n",
        "### **7Ô∏è‚É£ Example Algorithms**\n",
        "\n",
        "- **Bagging**:\n",
        "  - Common Bagging algorithms include:\n",
        "    - **Random Forests** (using decision trees as base learners)\n",
        "    - **Bagged Decision Trees**\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Common Boosting algorithms include:\n",
        "    - **AdaBoost** (Adaptive Boosting)\n",
        "    - **Gradient Boosting** (including **XGBoost**, **LightGBM**, **CatBoost**)\n",
        "    - **LogitBoost**\n",
        "\n",
        "---\n",
        "\n",
        "### **8Ô∏è‚É£ Strengths and Weaknesses**\n",
        "\n",
        "- **Bagging**:\n",
        "  - **Strengths**:\n",
        "    - Reduces overfitting by lowering variance.\n",
        "    - Works well when the base model is prone to overfitting (e.g., decision trees).\n",
        "    - Suitable for parallel processing, making it scalable.\n",
        "  - **Weaknesses**:\n",
        "    - Might not perform well when the base model has high bias or is weak (Bagging can't improve weak models much).\n",
        "\n",
        "- **Boosting**:\n",
        "  - **Strengths**:\n",
        "    - Great at reducing bias and improving the performance of weak models.\n",
        "    - Often results in higher accuracy compared to Bagging, especially in the case of weak learners.\n",
        "  - **Weaknesses**:\n",
        "    - Can be **sensitive to noisy data** and outliers, as Boosting tries to fit all data points, including noisy ones.\n",
        "    - Training is slower because of sequential model training.\n",
        "    - More prone to overfitting if the number of iterations is too high.\n",
        "\n",
        "---\n",
        "\n",
        "### **9Ô∏è‚É£ Summary of Key Differences**\n",
        "\n",
        "| **Feature**                  | **Bagging**                           | **Boosting**                             |\n",
        "|------------------------------|---------------------------------------|------------------------------------------|\n",
        "| **Objective**                 | Reduce variance, improve stability    | Reduce bias, improve model accuracy     |\n",
        "| **Training Process**          | Parallel, independent models          | Sequential, dependent models            |\n",
        "| **Sampling**                  | Random sampling with replacement      | Weights are adjusted based on misclassification |\n",
        "| **Model Weighting**           | Equal contribution of all models      | Models weighted based on performance    |\n",
        "| **Base Learner**              | Typically high-variance models (e.g., decision trees) | Weak models (e.g., shallow trees)       |\n",
        "| **Training Speed**            | Faster (parallelizable)               | Slower (sequential)                     |\n",
        "| **Examples**                  | Random Forest, Bagged Trees           | AdaBoost, Gradient Boosting, XGBoost    |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Bagging vs Boosting**\n",
        "\n",
        "- **Use Bagging**:\n",
        "  - When the base model is prone to **high variance**, such as decision trees.\n",
        "  - When you have a large dataset and want to speed up training with parallelization.\n",
        "  - When you want a **robust, stable model** that performs well across various data scenarios.\n",
        "  \n",
        "- **Use Boosting**:\n",
        "  - When the base model is weak and you need to **improve its performance**.\n",
        "  - When you want to focus on **improving accuracy** and can tolerate longer training times.\n",
        "  - When you're working with complex, **imbalanced datasets** or datasets with more noise.\n",
        "\n",
        "---\n",
        "\n",
        "In summary, **Bagging** is best when reducing variance and improving stability is the goal, while **Boosting** is ideal for improving the performance of weak learners and reducing bias. Each technique has its strengths, and the choice between them depends on the problem at hand and the specific challenges you're trying to address.\n"
      ],
      "metadata": {
        "id": "iPjAqT2BhQCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dj-ATwbeh1dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?"
      ],
      "metadata": {
        "id": "r_70OTLxh7pp"
      }
    },
    {
      "source": [
        "# Importing required libraries for training a Bagging Classifier using Decision Trees\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset as an example\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as the base classifier\n",
        "# The parameter name has been changed to 'estimator' in recent versions\n",
        "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                               n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVVsnIati0Tb",
        "outputId": "57669e75-aa9f-4e61-fffe-dd9793fdac05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "qRlGMglVi77w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries for training a Bagging Regressor using Decision Trees\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "# Load the Boston housing dataset as an example\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees as the base regressor\n",
        "bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeClassifier(),\n",
        "                                    n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Regressor\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mse\n"
      ],
      "metadata": {
        "id": "KPzLkvV-jpRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "elxOQpcCjt86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "accuracy, feature_importances\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFTYBzq_j5YQ",
        "outputId": "e6959a82-e215-4c3e-83bc-4387f8b2620b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9649122807017544,\n",
              " array([0.04870337, 0.01359088, 0.05326975, 0.04755501, 0.00728533,\n",
              "        0.01394433, 0.06800084, 0.10620999, 0.00377029, 0.00388577,\n",
              "        0.02013892, 0.00472399, 0.01130301, 0.02240696, 0.00427091,\n",
              "        0.00525322, 0.00938583, 0.00351326, 0.00401842, 0.00532146,\n",
              "        0.07798688, 0.02174901, 0.06711483, 0.15389236, 0.01064421,\n",
              "        0.02026604, 0.0318016 , 0.14466327, 0.01012018, 0.00521012]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "8ZeVTshkkAPO"
      }
    },
    {
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor\n",
        "\n",
        "# Generate some sample regression data\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X_reg, y_reg = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "dt_predictions = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error for both models\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "\n",
        "rf_mse, dt_mse"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcdGiUzWkYoZ",
        "outputId": "9427c0ea-9d02-4833-9283-9be7d0ee037f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21452.89253678296, 31002.75146553326)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "q6F51-upkfNC"
      }
    },
    {
      "source": [
        "# Initialize and train the Random Forest Classifier with OOB scoring enabled\n",
        "# Load the breast cancer dataset (or any other dataset suitable for classification)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into train-test sets (make sure to use the classification dataset)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_classifier_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf_classifier_oob.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB score\n",
        "oob_score = rf_classifier_oob.oob_score_\n",
        "\n",
        "oob_score"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfnSr98Mk_PX",
        "outputId": "f28b12c1-facf-4571-c8e7-0cd57ef082c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9560439560439561"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26.Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "qHs5UHEalFNk"
      }
    },
    {
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the base estimator (SVM)\n",
        "svm_estimator = SVC(random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with SVM as the base estimator\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_classifier = BaggingClassifier(estimator=svm_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "bagging_accuracy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AVtdkYAldvt",
        "outputId": "df32e4a4-509e-42cf-c504-2b9fec522b39"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9473684210526315"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27.Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "uafjFGnelinH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the accuracy scores for different numbers of trees\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate Random Forest Classifier with different numbers of trees\n",
        "for n_estimators in n_estimators_list:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append((n_estimators, accuracy))\n",
        "\n",
        "accuracies\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9kU6Fh4lpDJ",
        "outputId": "6c02c07b-e72e-4487-d101-e75a5d114aca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(10, 0.956140350877193),\n",
              " (50, 0.9649122807017544),\n",
              " (100, 0.9649122807017544),\n",
              " (200, 0.9649122807017544)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "WHcD7aRblzz6"
      }
    },
    {
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Initialize the base estimator (Logistic Regression)\n",
        "logreg_estimator = LogisticRegression(random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with Logistic Regression as the base estimator\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_classifier_logreg = BaggingClassifier(estimator=logreg_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier_logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = bagging_classifier_logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "auc_score"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQiFTW3dmR5T",
        "outputId": "4c51ab2b-d100-46b7-fd2f-bfafcba73983"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9980347199475925)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "0wL56_ojmXh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate some sample regression data\n",
        "X_reg, y_reg = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "feature_importances\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdyJByVNmcfw",
        "outputId": "c7da9d66-aadf-4aec-adb2-68613abff917"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03986177, 0.04188141, 0.02495076, 0.07568168, 0.21036215,\n",
              "       0.21059174, 0.2737618 , 0.0388154 , 0.01672388, 0.06736939])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "M9OhSv70mnD2"
      }
    },
    {
      "source": [
        "# Initialize the base classifier (Random Forest) for Bagging\n",
        "rf_classifier_base = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Random Forest as the base estimator\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_classifier = BaggingClassifier(estimator=rf_classifier_base, n_estimators=50, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IQaSxGaNm-XV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "xih8SflunDYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best accuracy score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "best_params, best_score\n"
      ],
      "metadata": {
        "id": "Md8nSHMNjr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q32.Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "jP9VUnIzjv3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different numbers of base estimators to test\n",
        "n_estimators_list = [1, 5, 10, 20, 50, 100]\n",
        "\n",
        "# Store results\n",
        "mse_scores = []\n",
        "r2_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressor with different numbers of base estimators\n",
        "for n_estimators in n_estimators_list:\n",
        "    model = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                             n_estimators=n_estimators,\n",
        "                             random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    mse_scores.append(mse)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "# Plot results\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "ax1.set_xlabel(\"Number of Base Estimators\")\n",
        "ax1.set_ylabel(\"Mean Squared Error\", color=\"tab:red\")\n",
        "ax1.plot(n_estimators_list, mse_scores, marker=\"o\", color=\"tab:red\", label=\"MSE\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"R¬≤ Score\", color=\"tab:blue\")\n",
        "ax2.plot(n_estimators_list, r2_scores, marker=\"s\", color=\"tab:blue\", label=\"R¬≤ Score\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
        "\n",
        "plt.title(\"Bagging Regressor Performance vs. Number of Base Estimators\")\n",
        "plt.show()\n",
        "\n",
        "# Return results\n",
        "list(zip(n_estimators_list, mse_scores, r2_scores))\n"
      ],
      "metadata": {
        "id": "RVk_s03skYPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q33.Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "OL0pi_PLka5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_idx = np.where(y_test != y_pred)[0]\n",
        "misclassified_samples = pd.DataFrame(X_test[misclassified_idx], columns=data.feature_names)\n",
        "misclassified_samples[\"True Label\"] = y_test[misclassified_idx]\n",
        "misclassified_samples[\"Predicted Label\"] = y_pred[misclassified_idx]\n",
        "\n",
        "accuracy, conf_matrix, misclassified_samples\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0CnwmV4kf7G",
        "outputId": "92ced8b8-6f26-4c03-e4b8-a92c00d446ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9,\n",
              " array([[10,  0,  0],\n",
              "        [ 0,  9,  1],\n",
              "        [ 0,  2,  8]]),\n",
              "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              " 0                6.0               3.0                4.8               1.8   \n",
              " 1                6.1               2.6                5.6               1.4   \n",
              " 2                6.7               3.0                5.0               1.7   \n",
              " \n",
              "    True Label  Predicted Label  \n",
              " 0           2                1  \n",
              " 1           2                1  \n",
              " 2           1                2  )"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "VM_YCK7tktct"
      }
    },
    {
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees as base estimators\n",
        "# Replace 'base_estimator' with 'estimator'\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "conf_matrix_bagging = confusion_matrix(y_test, y_pred_bagging)\n",
        "\n",
        "accuracy_dt, conf_matrix_dt, accuracy_bagging, conf_matrix_bagging"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLg_ram4lNxE",
        "outputId": "91f71f02-7f52-4715-af73-67fc6a0fb9cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9333333333333333,\n",
              " array([[10,  0,  0],\n",
              "        [ 0,  9,  1],\n",
              "        [ 0,  1,  9]]),\n",
              " 0.9666666666666667,\n",
              " array([[10,  0,  0],\n",
              "        [ 0,  9,  1],\n",
              "        [ 0,  0, 10]]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q35.Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "t7JOccxilUW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix_rf, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n",
        "\n",
        "# Return accuracy\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "accuracy_rf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "kAsuOCsplZpD",
        "outputId": "67242dec-3867-4b1b-861e-1a749d2b0f1a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "PqMNLCKWlnvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define base learners\n",
        "base_learners = [\n",
        "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('logistic', LogisticRegression(max_iter=200, random_state=42))\n",
        "]\n",
        "\n",
        "# Define Stacking Classifier with Logistic Regression as meta-learner\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train and evaluate each model\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=200, random_state=42),\n",
        "    \"Stacking Classifier\": stacking_clf\n",
        "}\n",
        "\n",
        "# Store accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[name] = accuracy\n",
        "\n",
        "# Print accuracy results\n",
        "for model_name, acc in accuracy_results.items():\n",
        "    print(f\"{model_name}: Accuracy = {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdhrRyZklvwp",
        "outputId": "92bc5e99-2ad0-49ae-9c32-e1b2a5758e8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree: Accuracy = 0.9333\n",
            "SVM: Accuracy = 0.9667\n",
            "Logistic Regression: Accuracy = 0.9667\n",
            "Stacking Classifier: Accuracy = 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q37.Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "eFzLM172mBgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_E8kr7imD0r",
        "outputId": "6d7649cf-0ddc-40ee-fbd6-41076f3d79fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "             Feature  Importance\n",
            "3   petal width (cm)    0.437185\n",
            "2  petal length (cm)    0.431466\n",
            "0  sepal length (cm)    0.116349\n",
            "1   sepal width (cm)    0.015000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "csC5qT8kmL1n"
      }
    },
    {
      "source": [
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), # Change 'base_estimator' to 'estimator'\n",
        "                                  n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "a036tFLLmfyF",
        "outputId": "3518ac6c-5aa2-4a63-f2bc-be76afa55764"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n",
              "                  random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n",
              "                  random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BaggingClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.BaggingClassifier.html\">?<span>Documentation for BaggingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n",
              "                  random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: DecisionTreeClassifier</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier()</pre></div> </div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "vp8_CDZcmt2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Test different values of max_depth\n",
        "max_depth_values = [1, 2, 3, 5, 10, None]  # None means unlimited depth\n",
        "accuracy_scores = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plot accuracy vs max_depth\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot([str(d) for d in max_depth_values], accuracy_scores, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy results\n",
        "for depth, acc in zip(max_depth_values, accuracy_scores):\n",
        "    print(f\"Max Depth: {depth}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "FpJRClsJm0Se",
        "outputId": "a05649d0-fac7-47c4-d71e-c32ce36310ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: 1, Accuracy: 0.9667\n",
            "Max Depth: 2, Accuracy: 0.9000\n",
            "Max Depth: 3, Accuracy: 0.9667\n",
            "Max Depth: 5, Accuracy: 0.9333\n",
            "Max Depth: 10, Accuracy: 0.9000\n",
            "Max Depth: None, Accuracy: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance."
      ],
      "metadata": {
        "id": "3Gc0vrKUm_48"
      }
    },
    {
      "source": [
        "# Train Bagging Regressors\n",
        "bagging_dt = BaggingRegressor(estimator=dt_regressor, n_estimators=50, random_state=42)  # Change 'base_estimator' to 'estimator'\n",
        "bagging_knn = BaggingRegressor(estimator=knn_regressor, n_estimators=50, random_state=42)  # Change 'base_estimator' to 'estimator'\n",
        "\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "bagging_knn.fit(X_train, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "y8x7wERPnVXT",
        "outputId": "f2440937-e27f-4e17-9bf6-2d503644e889"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50,\n",
              "                 random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"‚ñ∏\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"‚ñæ\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50,\n",
              "                 random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>BaggingRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.BaggingRegressor.html\">?<span>Documentation for BaggingRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50,\n",
              "                 random_state=42)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: KNeighborsRegressor</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsRegressor()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\">?<span>Documentation for KNeighborsRegressor</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsRegressor()</pre></div> </div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "8_b0sjw1ncHL"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Binarize labels for multi-class ROC-AUC calculation\n",
        "y_bin = label_binarize(y, classes=[0, 1, 2])\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_scores = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Binarize the test set labels (y_test) instead of the entire dataset (y)\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "\n",
        "# Compute ROC-AUC score (macro-averaged) using y_test_bin\n",
        "roc_auc = roc_auc_score(y_test_bin, y_scores, multi_class=\"ovr\", average=\"macro\")\n",
        "print(f\"Random Forest ROC-AUC Score (Macro-Averaged): {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve (only for binary classification)\n",
        "# ... (rest of the code)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddI3yZKHn0eq",
        "outputId": "3d6adc17-e8c2-4c02-d18c-2dc07f4a928e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest ROC-AUC Score (Macro-Averaged): 0.9867\n"
          ]
        }
      ]
    }
  ]
}