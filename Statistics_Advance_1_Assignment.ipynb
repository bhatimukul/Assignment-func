{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4P4FhkV7Z/5E/nezRQHvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatimukul/Assignment-func/blob/main/Statistics_Advance_1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "t4jnPZ_XJxJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**The F-distribution, also known as Fisher's F-distribution, is a continuous probability distribution commonly used in statistical tests, particularly the analysis of variance (ANOVA) and regression analysis. Here are the key properties of the F-distribution:\n",
        "\n",
        "1. **Definition and Purpose**:\n",
        "   - The F-distribution represents the ratio of two independent chi-squared distributed random variables divided by their respective degrees of freedom.\n",
        "   - It is primarily used to compare variances and determine if two datasets have significantly different variances or test the overall significance of regression models.\n",
        "\n",
        "2. **Degrees of Freedom**:\n",
        "   - The F-distribution is defined by two degrees of freedom parameters: \\( d_1 \\) (numerator degrees of freedom) and \\( d_2 \\) (denominator degrees of freedom).\n",
        "   - These parameters are associated with the two sample variances (or chi-squared distributions) being compared. The shape of the F-distribution depends on these degrees of freedom.\n",
        "\n",
        "3. **Shape**:\n",
        "   - The shape of the F-distribution is asymmetric, positively skewed, and non-negative, meaning it only takes positive values.\n",
        "   - As the degrees of freedom increase, the distribution becomes less skewed and more symmetric, approaching a normal distribution.\n",
        "\n",
        "4. **Range**:\n",
        "   - The F-distribution only has values in the range \\([0, \\infty)\\). It cannot be negative because it represents a ratio of variances (both of which are non-negative).\n",
        "\n",
        "5. **Mean and Variance**:\n",
        "   - The mean of the F-distribution is approximately \\( \\frac{d_2}{d_2 - 2} \\), provided \\( d_2 > 2 \\).\n",
        "   - The variance of the F-distribution is \\( \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)} \\), provided \\( d_2 > 4 \\).\n",
        "\n",
        "6. **Right-Skewed**:\n",
        "   - The F-distribution is right-skewed, especially for small degrees of freedom in the numerator or denominator. The right tail is longer, which is critical in hypothesis testing as it allows for evaluating the likelihood of observing large ratios of variances.\n",
        "\n",
        "7. **Applications**:\n",
        "   - **ANOVA**: In analysis of variance, the F-distribution is used to compare variances among groups to determine if group means are significantly different.\n",
        "   - **Regression Analysis**: It tests the overall significance of a regression model by comparing the explained variance to unexplained variance.\n",
        "   - **Comparing Variances**: The F-test is used to test the hypothesis that two populations have equal variances.\n",
        "\n",
        "8. **Critical Values and Hypothesis Testing**:\n",
        "   - Critical values of the F-distribution are typically used in the right tail of the distribution for testing purposes. The critical value is determined by the chosen significance level (e.g., 0.05) and the degrees of freedom associated with the test.\n",
        "   - If the calculated F-statistic from a sample exceeds the critical F-value, the null hypothesis (e.g., of equal variances or no effect) is rejected.\n",
        "\n"
      ],
      "metadata": {
        "id": "-loDGetCKA27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **Q2.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "H0kgkcUnK136"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**The F-distribution is used in various statistical tests, primarily because it is well-suited to situations that involve comparing variances. Here are some key types of tests where the F-distribution is commonly applied and why it is appropriate:\n",
        "\n",
        "### 1. **Analysis of Variance (ANOVA)**\n",
        "   - **Purpose**: ANOVA tests determine if there are statistically significant differences among the means of three or more groups.\n",
        "   - **Why F-Distribution Is Appropriate**: The F-distribution helps compare the variance among group means (explained variance) to the variance within groups (unexplained variance). If the ratio of these variances (F-statistic) is sufficiently high, it suggests that the differences among group means are unlikely to be due to random chance.\n",
        "\n",
        "### 2. **Regression Analysis (Overall Model Significance)**\n",
        "   - **Purpose**: In regression, the F-test assesses whether the model as a whole has predictive power or if all regression coefficients are zero (indicating no relationship between predictors and the outcome variable).\n",
        "   - **Why F-Distribution Is Appropriate**: In multiple regression, the F-statistic compares the variance explained by the model to the residual (error) variance. A higher F-value suggests the model explains a significant portion of the variance in the outcome variable, which justifies keeping the predictors.\n",
        "\n",
        "### 3. **Comparison of Two Variances (F-test for Variance)**\n",
        "   - **Purpose**: This test evaluates if two populations have equal variances, often as an assumption check for other statistical tests (e.g., t-tests).\n",
        "   - **Why F-Distribution Is Appropriate**: The F-distribution directly represents the ratio of two sample variances. By calculating an F-statistic as the ratio of the larger sample variance to the smaller, researchers can determine if the observed difference in variances is statistically significant.\n",
        "\n",
        "### 4. **Post-hoc Tests in ANOVA (e.g., Tukey's test, Bonferroni correction)**\n",
        "   - **Purpose**: After finding a significant effect in ANOVA, post-hoc tests examine specific group differences.\n",
        "   - **Why F-Distribution Is Appropriate**: Post-hoc tests often use the F-distribution to control for the family-wise error rate when making multiple comparisons, maintaining the integrity of the statistical inference.\n",
        "\n",
        "### 5. **Tests for Comparing Nested Models (e.g., Likelihood Ratio Tests)**\n",
        "   - **Purpose**: These tests compare two models where one is a special case (or subset) of the other, to see if adding more parameters significantly improves model fit.\n",
        "   - **Why F-Distribution Is Appropriate**: The F-test assesses whether the increase in explained variance due to added parameters is statistically significant, helping to balance model complexity with fit quality.\n",
        "\n",
        "In each of these cases, the F-distribution is appropriate because it represents the ratio of variances, which is central to assessing whether observed differences are statistically significant. Its right-skewed shape helps capture the rare, extreme values in the upper tail, which indicate unlikely scenarios under the null hypothesis. This characteristic is particularly useful when testing for significant effects."
      ],
      "metadata": {
        "id": "o81xn_4BLODz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.What are the key assumptions required for conducting an F-test to compare the variances of two**\n",
        "**populations?**"
      ],
      "metadata": {
        "id": "BFmGddmtMB98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**Conducting an F-test to compare the variances of two populations requires several key assumptions to ensure valid results. Here are the primary assumptions for an F-test:\n",
        "\n",
        "1. **Independence of Samples**:\n",
        "   - The two samples being compared should be independent of each other. This means that the values in one sample should not influence or relate to the values in the other sample.\n",
        "\n",
        "2. **Normality**:\n",
        "   - Each population from which the samples are drawn should follow a normal distribution. The F-test is sensitive to deviations from normality, and significant skewness or kurtosis can affect the validity of the test results.\n",
        "   - If the populations are not normal, the F-test may yield misleading results, especially with small sample sizes.\n",
        "\n",
        "3. **Scale of Measurement**:\n",
        "   - The data should be measured on an interval or ratio scale. This ensures that calculating variances is meaningful and that the ratio of variances (which the F-test evaluates) is interpretable.\n",
        "\n",
        "4. **Random Sampling**:\n",
        "   - Each sample should be a random sample from its respective population. Random sampling helps ensure that each sample is representative of its population, reducing potential biases that could affect the comparison of variances.\n",
        "\n",
        "5. **Homogeneity of Variance** (for certain types of F-tests in ANOVA):\n",
        "   - While this assumption does not apply to a simple two-sample F-test comparing variances, it is essential when using F-tests as part of ANOVA, where it is assumed that variances within groups are equal.\n",
        "   \n",
        "When these assumptions are met, the F-test can provide a valid comparison of the variances from two populations. If normality is questionable, alternative tests such as the Levene's test or Brown-Forsythe test may be more robust for comparing variances, especially with non-normally distributed data."
      ],
      "metadata": {
        "id": "una66RTmMOV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "AWgnVyWYMgeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**The purpose of Analysis of Variance (ANOVA) is to test for statistically significant differences among the means of three or more groups. It helps determine if at least one group mean is significantly different from the others, which can indicate an effect or association among the groups being compared. Here’s a breakdown of ANOVA’s purpose and how it differs from a t-test:\n",
        "\n",
        "### Purpose of ANOVA\n",
        "- **Testing Mean Differences Across Multiple Groups**: ANOVA is designed to compare means across three or more groups simultaneously to avoid the need for multiple pairwise comparisons, which could increase the risk of Type I error (false positives).\n",
        "- **Partitioning Variance**: ANOVA decomposes the total variance into \"between-group\" variance (differences due to group membership) and \"within-group\" variance (differences within each group), helping to determine if the between-group variance is large relative to the within-group variance.\n",
        "- **Single Test for Multiple Comparisons**: ANOVA allows researchers to assess differences among all groups with a single test, providing a streamlined analysis and maintaining the significance level (e.g., 0.05) without inflation that would occur with multiple t-tests.\n",
        "\n",
        "### How ANOVA Differs from a t-Test\n",
        "1. **Number of Groups Compared**:\n",
        "   - **t-Test**: Compares the means of only two groups.\n",
        "   - **ANOVA**: Compares the means of three or more groups, making it a more versatile test for experimental designs with multiple conditions.\n",
        "\n",
        "2. **Multiple Comparisons and Type I Error Control**:\n",
        "   - **t-Test**: If comparing more than two groups, multiple pairwise t-tests are required (e.g., Group 1 vs. Group 2, Group 1 vs. Group 3, etc.). This approach increases the chance of Type I error due to repeated testing.\n",
        "   - **ANOVA**: By analyzing all groups together in one test, ANOVA controls for Type I error, maintaining the overall significance level. If ANOVA finds significant differences, post-hoc tests can then specify which groups differ.\n",
        "\n",
        "3. **Test Statistic**:\n",
        "   - **t-Test**: Uses the t-statistic, calculated as the difference between two sample means divided by their pooled standard error.\n",
        "   - **ANOVA**: Uses the F-statistic, calculated as the ratio of the variance between groups to the variance within groups. A large F-statistic suggests that between-group variance is greater than within-group variance, indicating significant mean differences.\n",
        "\n",
        "4. **Hypotheses Tested**:\n",
        "   - **t-Test**: Tests if the means of two groups are equal (null hypothesis: \\( \\mu_1 = \\mu_2 \\)).\n",
        "   - **ANOVA**: Tests if at least one group mean differs from the others (null hypothesis: \\( \\mu_1 = \\mu_2 = \\mu_3 = ... = \\mu_k \\)), where \\( k \\) is the number of groups.\n",
        "\n",
        "### When to Use ANOVA vs. t-Test\n",
        "- **Use a t-test** when you are comparing exactly two groups (e.g., treatment vs. control).\n",
        "- **Use ANOVA** when you are comparing three or more groups, as it provides a comprehensive, single test for assessing overall group differences without inflating Type I error."
      ],
      "metadata": {
        "id": "U8nHy1_wMrQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5.Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more**\n",
        "**than two groups.**"
      ],
      "metadata": {
        "id": "IoV6vL56NCDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**When comparing more than two groups, a **one-way ANOVA** is generally preferred over multiple t-tests. Here’s an explanation of when and why one-way ANOVA is used in these situations:\n",
        "\n",
        "### When to Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "- **When there are more than two groups to compare**: One-way ANOVA is designed specifically for testing differences among three or more group means.\n",
        "- **When you want to avoid inflating the Type I error rate**: Each time a t-test is conducted, there’s a chance of a Type I error (incorrectly rejecting the null hypothesis). Conducting multiple t-tests (one for each pair of groups) increases the cumulative probability of making at least one Type I error.\n",
        "\n",
        "### Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "1. **Control for Type I Error**:\n",
        "   - Each t-test conducted has a significance level (alpha) — typically 0.05. For multiple t-tests, the risk of at least one false positive (Type I error) increases. For example, if comparing three groups, three t-tests are needed, and for four groups, six t-tests are needed, increasing the chances of error further.\n",
        "   - **One-way ANOVA** addresses this by providing a single test for differences among all group means, thus maintaining the overall significance level (alpha) at the desired rate (e.g., 0.05).\n",
        "\n",
        "2. **Efficiency**:\n",
        "   - One-way ANOVA allows for comparing multiple groups simultaneously rather than performing multiple pairwise t-tests, streamlining the analysis process.\n",
        "   - This single-test approach saves time and effort and avoids unnecessary comparisons.\n",
        "\n",
        "3. **Detection of Overall Differences**:\n",
        "   - One-way ANOVA determines whether there is a significant difference among the means of the groups as a whole, rather than just between specific pairs. If ANOVA shows a significant result, post-hoc tests can then identify which specific pairs of groups differ, providing a more detailed analysis if needed.\n",
        "\n",
        "4. **Interpretability and Statistical Power**:\n",
        "   - By using one-way ANOVA, researchers can interpret a single F-statistic that represents the ratio of between-group variance to within-group variance. This approach is statistically more powerful and avoids redundancy compared to conducting numerous t-tests, as it uses the total dataset to estimate variability.\n",
        "   \n",
        "### Summary\n",
        "In essence, **one-way ANOVA** is preferred when comparing more than two groups because it controls for Type I error, is more efficient, provides a comprehensive test of group differences, and increases the interpretability and statistical power of the analysis."
      ],
      "metadata": {
        "id": "uIPeDjJ6NjHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.**\n",
        "**How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "bAvCrvlPOLSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**In ANOVA, variance is partitioned into **between-group variance** and **within-group variance** to analyze how much of the total variation in the data is due to differences among group means versus differences within the groups themselves. This partitioning is central to calculating the **F-statistic**, which is used to assess whether there are significant differences between group means. Here’s a breakdown of how this process works:\n",
        "\n",
        "### Partitioning Variance in ANOVA\n",
        "\n",
        "1. **Total Variance (Total Sum of Squares, SST)**:\n",
        "   - Total variance represents the overall variation of all observations around the grand mean (the mean of all data points across all groups).\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_{\\text{grand}})^2\n",
        "     \\]\n",
        "   - Here, \\( X_{ij} \\) is an individual observation, \\( \\bar{X}_{\\text{grand}} \\) is the overall mean across all groups, \\( k \\) is the number of groups, and \\( n_i \\) is the number of observations in each group.\n",
        "\n",
        "2. **Between-Group Variance (Sum of Squares Between, SSB)**:\n",
        "   - This component measures the variation between group means and the grand mean, reflecting how different each group mean is from the overall mean. It’s the part of the variance explained by the group membership.\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{SSB} = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X}_{\\text{grand}})^2\n",
        "     \\]\n",
        "   - Here, \\( \\bar{X}_i \\) is the mean of group \\( i \\), and \\( n_i \\) is the number of observations in that group. Larger differences between group means and the grand mean lead to a higher SSB, indicating greater between-group variance.\n",
        "\n",
        "3. **Within-Group Variance (Sum of Squares Within, SSW)**:\n",
        "   - This component represents the variation within each group, measuring how much individual observations deviate from their respective group mean. It reflects variability that is not explained by group membership and is sometimes referred to as error variance.\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2\n",
        "     \\]\n",
        "   - This is the sum of squared deviations of each observation from its group mean.\n",
        "\n",
        "### Calculation of the F-Statistic Using Partitioned Variance\n",
        "\n",
        "The F-statistic is calculated as a ratio of the **between-group mean square** (MSB) to the **within-group mean square** (MSW):\n",
        "\n",
        "1. **Mean Square Between (MSB)**:\n",
        "   - The mean square between is found by dividing the between-group sum of squares (SSB) by its degrees of freedom (\\( k - 1 \\), where \\( k \\) is the number of groups).\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{MSB} = \\frac{\\text{SSB}}{k - 1}\n",
        "     \\]\n",
        "\n",
        "2. **Mean Square Within (MSW)**:\n",
        "   - The mean square within is calculated by dividing the within-group sum of squares (SSW) by its degrees of freedom (\\( N - k \\), where \\( N \\) is the total number of observations and \\( k \\) is the number of groups).\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{MSW} = \\frac{\\text{SSW}}{N - k}\n",
        "     \\]\n",
        "\n",
        "3. **F-Statistic**:\n",
        "   - The F-statistic is the ratio of the mean square between (MSB) to the mean square within (MSW).\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
        "     \\]\n",
        "   - If the between-group variance is much larger than the within-group variance (i.e., \\( F \\) is large), this suggests that the differences among group means are likely not due to random chance.\n",
        "\n",
        "### Interpreting the F-Statistic\n",
        "- A larger F-statistic indicates that a greater proportion of the total variance is explained by between-group differences compared to within-group variability.\n",
        "- In hypothesis testing, the F-statistic is compared to a critical value from the F-distribution, determined by the degrees of freedom for between-group and within-group variances. If the F-statistic exceeds the critical value, the null hypothesis (that all group means are equal) is rejected, indicating significant differences between at least some group means.\n",
        "\n",
        "In summary, partitioning variance into between-group and within-group components allows ANOVA to isolate the effect of group differences from random variation within groups. This partitioning provides the basis for the F-statistic, which quantifies whether observed differences between group means are statistically significant."
      ],
      "metadata": {
        "id": "oAnZjMnUP-Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7.Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key**\n",
        "**differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "n57dUqzqQeA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans.**The classical (frequentist) and Bayesian approaches to ANOVA both aim to test for differences among group means, but they differ fundamentally in how they handle uncertainty, parameter estimation, and hypothesis testing. Here’s a comparison of the two approaches:\n",
        "\n",
        "### 1. Handling of Uncertainty\n",
        "   - **Frequentist Approach**:\n",
        "     - In classical ANOVA, uncertainty is assessed through **sampling distributions** and **p-values**. Under the null hypothesis (that group means are equal), the probability of observing the calculated F-statistic or a more extreme value is used to determine significance.\n",
        "     - Confidence intervals for parameters (e.g., group means) represent ranges that, under repeated sampling, would contain the true parameter values a specified percentage of the time (e.g., 95%).\n",
        "\n",
        "   - **Bayesian Approach**:\n",
        "     - Bayesian ANOVA treats parameters as random variables and quantifies uncertainty in terms of **posterior distributions**. The posterior distribution is obtained by updating prior beliefs with observed data, using Bayes’ theorem.\n",
        "     - Credible intervals, rather than confidence intervals, are used to express uncertainty. For example, a 95% credible interval represents the range within which the true parameter lies with a 95% probability, given the observed data and prior.\n",
        "\n",
        "### 2. Parameter Estimation\n",
        "   - **Frequentist Approach**:\n",
        "     - Parameters (e.g., group means, variances) are estimated as fixed quantities (point estimates) derived solely from the observed data. Estimators like the mean and variance are computed from sample statistics.\n",
        "     - Frequentist methods rely on the sample mean and variance for each group without incorporating prior information.\n",
        "\n",
        "   - **Bayesian Approach**:\n",
        "     - Bayesian methods incorporate prior beliefs (prior distributions) about parameters before observing the data. These priors can be based on prior knowledge, expert opinion, or previous studies.\n",
        "     - The data updates these priors to produce posterior distributions for parameters, reflecting the combination of prior information and the observed data. This approach allows Bayesian ANOVA to \"shrink\" estimates toward prior means, especially when sample sizes are small or data is sparse.\n",
        "\n",
        "### 3. Hypothesis Testing\n",
        "   - **Frequentist Approach**:\n",
        "     - Hypothesis testing in classical ANOVA uses the F-test to assess whether the group means differ significantly. The null hypothesis assumes that all group means are equal, and significance is determined by comparing the F-statistic to a critical value from the F-distribution.\n",
        "     - P-values are central in this approach: a low p-value (e.g., < 0.05) suggests rejection of the null hypothesis, indicating that at least one group mean is significantly different.\n",
        "     - This approach is **binary**: it either rejects or fails to reject the null hypothesis, with no probabilistic interpretation of the hypotheses themselves.\n",
        "\n",
        "   - **Bayesian Approach**:\n",
        "     - Bayesian ANOVA does not rely on p-values or F-tests. Instead, it evaluates the probability of hypotheses (e.g., different models) by comparing their posterior probabilities. This allows for **probabilistic interpretation of hypotheses**, giving a direct probability that a hypothesis (such as differences in group means) is true, based on the observed data.\n",
        "     - Bayesian methods also allow model comparison using metrics like **Bayes factors**, which quantify the relative evidence for one model over another (e.g., a model where means differ vs. one where they are the same).\n",
        "     - Rather than a binary decision, Bayesian inference provides a measure of support for each hypothesis, allowing for a more nuanced understanding.\n",
        "\n",
        "### 4. Interpretation of Results\n",
        "   - **Frequentist Approach**:\n",
        "     - Results in frequentist ANOVA are interpreted through significance testing. If the F-test is significant, the conclusion is that at least one group mean differs from the others, though follow-up tests are needed to identify specific group differences.\n",
        "     - Confidence intervals can give insight into the range of plausible values for group means but do not directly indicate probability.\n",
        "\n",
        "   - **Bayesian Approach**:\n",
        "     - In Bayesian ANOVA, the interpretation is based on posterior probabilities and credible intervals. Credible intervals provide a range where a parameter likely lies with a certain probability, and Bayes factors offer a measure of support for one hypothesis over another.\n",
        "     - This approach provides a more intuitive interpretation of results, as it expresses the degree of belief in a hypothesis or parameter value directly.\n",
        "\n",
        "### Summary of Key Differences\n",
        "\n",
        "| Aspect                | Frequentist ANOVA                        | Bayesian ANOVA                               |\n",
        "|-----------------------|------------------------------------------|----------------------------------------------|\n",
        "| **Uncertainty**       | p-values, confidence intervals           | Posterior distributions, credible intervals  |\n",
        "| **Parameter Estimation** | Point estimates, relies on sample data  | Posterior distributions, combines prior and data |\n",
        "| **Hypothesis Testing**   | F-test, p-value significance            | Posterior probability, Bayes factors         |\n",
        "| **Interpretation**    | Binary reject/not reject conclusion      | Probabilistic support for hypotheses        |\n",
        "\n",
        "In summary, the Bayesian approach provides a flexible framework for ANOVA by incorporating prior information and yielding probabilistic statements about hypotheses and parameters. The frequentist approach, on the other hand, relies on sampling theory and provides p-values and confidence intervals for hypothesis testing, often interpreting results in a more binary fashion. Each approach has strengths, with the choice often depending on context, available prior knowledge, and preference for interpretive style."
      ],
      "metadata": {
        "id": "FMLWtsJGQvoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8.Question: You have two sets of data representing the incomes of two different professions:**\n",
        "\n",
        ".**Profession A: [48, 52, 55, 60, 62]**\n",
        "\n",
        ".**Profession B:[45, 50, 55, 52, 47] Perform an F-test to determine if the**\n",
        " **variances of the two professions incomes are equal. What are your conclusions based on the F-test?**\n",
        "\n",
        " Task:Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n"
      ],
      "metadata": {
        "id": "syEoGSJxRIJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Data for two professions\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate sample variances for each profession\n",
        "var_A = np.var(profession_A, ddof=1)  # Sample variance for Profession A\n",
        "var_B = np.var(profession_B, ddof=1)  # Sample variance for Profession B\n",
        "\n",
        "# Calculate the F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Degrees of freedom for each sample\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Calculate the p-value using the F-distribution (two-tailed test)\n",
        "p_value = 2 * min(f.cdf(F_statistic, df_A, df_B), 1 - f.cdf(F_statistic, df_A, df_B))\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2xBFu0MTvrb",
        "outputId": "80191c99-038b-435c-c629-190a197d0aa0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.49304859900533904)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-test results yield an F-statistic of approximately 2.09 and a p-value of 0.493.\n",
        "\n",
        "### Interpretation:\n",
        "Since the p-value (0.493) is much greater than the typical significance level of 0.05, we fail to reject the null hypothesis. This means there is no significant evidence to suggest that the variances of incomes between Profession A and Profession B are different. Therefore, we conclude that the variances in incomes for the two professions can be considered equal based on this test."
      ],
      "metadata": {
        "id": "ac-q1H4PT6-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9.Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:**\n",
        "\n",
        ".**Region A: [160, 162, 165, 158, 164]**\n",
        "\n",
        ".**Region B: [172, 175, 170, 168, 174]**\n",
        "\n",
        ".**Region C: [180, 182, 179, 185, 183]**\n",
        "\n",
        ".Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "\n",
        "\n",
        ".Objective: Learn how to perform one-way ANOVA using Python and interpret      \n",
        " F-statistic and p-value."
      ],
      "metadata": {
        "id": "_hSu3LUdT_tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Heights data for three different regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4bIKXcJVyiN",
        "outputId": "25e4232e-4f9f-4397-b5b6-7f1e792ab9b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one-way ANOVA results give an F-statistic of approximately 67.87 and a p-value of \\(2.87 \\times 10^{-7}\\).\n",
        "\n",
        "### Interpretation:\n",
        "Since the p-value is significantly lower than the common significance level of 0.05, we reject the null hypothesis. This indicates that there are statistically significant differences in average heights among the three regions. Therefore, we conclude that at least one region's average height differs from the others. Further post-hoc tests could be conducted to determine which specific regions differ."
      ],
      "metadata": {
        "id": "ZAAwKl0oV0mR"
      }
    }
  ]
}